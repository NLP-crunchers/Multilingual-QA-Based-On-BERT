{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "BERT_MLQA_TEST.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhQKHiaGaref",
        "outputId": "e9735295-4edf-4031-908c-ec929731a861"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R3APTcNRktA"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "class MLQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "train_large_dataset = pickle.load(open(\"train_large_dataset\", \"rb\")) # Augmented large dataset\n",
        "train_small_dataset = pickle.load(open(\"train_small_dataset\", \"rb\")) # Augmented small dataset\n",
        "train_zh_dataset = pickle.load(open(\"train_zh_dataset\", \"rb\")) # Large ONLY (without original) translated dataset\n",
        "\n",
        "train_dataset = pickle.load(open(\"train_dataset\", \"rb\"))\n",
        "val_dataset = pickle.load(open(\"val_dataset\", \"rb\"))\n",
        "zh_dataset = pickle.load(open(\"zh_dataset\", \"rb\"))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8KIPetBRktB"
      },
      "source": [
        "def compute_f1(predicted, true):\n",
        "    c = len(set(predicted) & set(true))\n",
        "    l1 = len(predicted)\n",
        "    l2 = len(true)\n",
        "    if(l1 + l2 == 0):\n",
        "        return 1\n",
        "    f1 = 2*c/(l1+l2)\n",
        "    return f1\n",
        "    \n",
        "def compute_em(predicted, true):\n",
        "    return int(predicted == true)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6hniE-aRktB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2807af5a-7551-4627-b4ff-0f0c1a4bc039"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, BertForQuestionAnswering\n",
        "# input_ids = None\n",
        "# attention_mask = None\n",
        "# start_positions = None\n",
        "# end_positions = None\n",
        "# model = None\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_small_dataset, batch_size=8, shuffle=True) # MODIFY dataset here !!!!!\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "zh_loader = DataLoader(zh_dataset, batch_size=8, shuffle=True)\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "dummy_index = 0\n",
        "collect = []\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "\n",
        "val_batch = 300\n",
        "max_epoch = 3\n",
        "train_batch = len(train_loader)\n",
        "for epoch in range(max_epoch):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        if batch_idx % val_batch == 0 or batch_idx == train_batch - 1:\n",
        "            print(\"Epoch: {}/{}, batch: {}/{}, {:%}\".format(epoch, max_epoch, batch_idx, train_batch, batch_idx/train_batch))\n",
        "            dummy_index = epoch * train_batch + batch_idx\n",
        "            model.eval()\n",
        "            eval_cnt = 0\n",
        "            F1 = 0.0\n",
        "            EM = 0.0\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                optim.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "                samples_in_batch = len(input_ids)\n",
        "                for i in range(samples_in_batch):\n",
        "                    predict_start = int(outputs[1][i].argmax().cpu())\n",
        "                    predict_end = int(outputs[2][i].argmax().cpu())\n",
        "                    true_start = int(start_positions[i].cpu())\n",
        "                    true_end = int(end_positions[i].cpu())\n",
        "                    F1 += compute_f1(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                    EM += compute_em(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                eval_cnt += samples_in_batch\n",
        "            F1 /= eval_cnt\n",
        "            EM /= eval_cnt\n",
        "            print(\"English eval score: F1:{}, EM:{}\".format(F1, EM))\n",
        "            eng_f1, eng_em = F1, EM\n",
        "\n",
        "            eval_cnt = 0\n",
        "            F1 = 0.0\n",
        "            EM = 0.0\n",
        "            for batch_idx, batch in enumerate(zh_loader):\n",
        "                optim.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "                samples_in_batch = len(input_ids)\n",
        "                for i in range(samples_in_batch):\n",
        "                    predict_start = int(outputs[1][i].argmax().cpu())\n",
        "                    predict_end = int(outputs[2][i].argmax().cpu())\n",
        "                    true_start = int(start_positions[i].cpu())\n",
        "                    true_end = int(end_positions[i].cpu())\n",
        "                    F1 += compute_f1(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                    EM += compute_em(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                eval_cnt += samples_in_batch\n",
        "            F1 /= eval_cnt\n",
        "            EM /= eval_cnt\n",
        "            print(\"Chinese eval score: F1:{}, EM:{}\".format(F1, EM))\n",
        "            print('collect data: ', eng_f1, eng_em, F1, EM, dummy_index, epoch)\n",
        "            collect.append((eng_f1, eng_em, F1, EM, dummy_index, epoch))\n",
        "        \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1749\n",
            "144\n",
            "Epoch: 0/3, batch: 0/1749, 0.000000%\n",
            "English eval score: F1:0.14156745529837858, EM:0.1210801393728223\n",
            "Chinese eval score: F1:0.09376655100388047, EM:0.0496031746031746\n",
            "collect data:  0.14156745529837858 0.1210801393728223 0.09376655100388047 0.0496031746031746 0 0\n",
            "Epoch: 0/3, batch: 300/1749, 17.152659%\n",
            "English eval score: F1:0.47069715377397175, EM:0.3797909407665505\n",
            "Chinese eval score: F1:0.31726979351435697, EM:0.2123015873015873\n",
            "collect data:  0.47069715377397175 0.3797909407665505 0.31726979351435697 0.2123015873015873 300 0\n",
            "Epoch: 0/3, batch: 600/1749, 34.305317%\n",
            "English eval score: F1:0.5520533827108367, EM:0.4416376306620209\n",
            "Chinese eval score: F1:0.41534611712857705, EM:0.29563492063492064\n",
            "collect data:  0.5520533827108367 0.4416376306620209 0.41534611712857705 0.29563492063492064 600 0\n",
            "Epoch: 0/3, batch: 900/1749, 51.457976%\n",
            "English eval score: F1:0.5814410989397145, EM:0.4712543554006969\n",
            "Chinese eval score: F1:0.4406542394912614, EM:0.32936507936507936\n",
            "collect data:  0.5814410989397145 0.4712543554006969 0.4406542394912614 0.32936507936507936 900 0\n",
            "Epoch: 0/3, batch: 1200/1749, 68.610635%\n",
            "English eval score: F1:0.6330149448883108, EM:0.5200348432055749\n",
            "Chinese eval score: F1:0.431014468816962, EM:0.32142857142857145\n",
            "collect data:  0.6330149448883108 0.5200348432055749 0.431014468816962 0.32142857142857145 1200 0\n",
            "Epoch: 0/3, batch: 1500/1749, 85.763293%\n",
            "English eval score: F1:0.6279643297365508, EM:0.5165505226480837\n",
            "Chinese eval score: F1:0.44370620279257894, EM:0.32142857142857145\n",
            "collect data:  0.6279643297365508 0.5165505226480837 0.44370620279257894 0.32142857142857145 1500 0\n",
            "Epoch: 0/3, batch: 1748/1749, 99.942824%\n",
            "English eval score: F1:0.6423175868658351, EM:0.5313588850174216\n",
            "Chinese eval score: F1:0.4675412888931693, EM:0.34325396825396826\n",
            "collect data:  0.6423175868658351 0.5313588850174216 0.4675412888931693 0.34325396825396826 1748 0\n",
            "Epoch: 1/3, batch: 0/1749, 0.000000%\n",
            "English eval score: F1:0.6387749695265992, EM:0.5287456445993032\n",
            "Chinese eval score: F1:0.4636390250786604, EM:0.3412698412698413\n",
            "collect data:  0.6387749695265992 0.5287456445993032 0.4636390250786604 0.3412698412698413 1749 1\n",
            "Epoch: 1/3, batch: 300/1749, 17.152659%\n",
            "English eval score: F1:0.63688822036043, EM:0.5252613240418118\n",
            "Chinese eval score: F1:0.46470791335996703, EM:0.3392857142857143\n",
            "collect data:  0.63688822036043 0.5252613240418118 0.46470791335996703 0.3392857142857143 2049 1\n",
            "Epoch: 1/3, batch: 600/1749, 34.305317%\n",
            "English eval score: F1:0.6388576598170083, EM:0.5235191637630662\n",
            "Chinese eval score: F1:0.46064881767192306, EM:0.3392857142857143\n",
            "collect data:  0.6388576598170083 0.5235191637630662 0.46064881767192306 0.3392857142857143 2349 1\n",
            "Epoch: 1/3, batch: 900/1749, 51.457976%\n",
            "English eval score: F1:0.6492661636739393, EM:0.5331010452961672\n",
            "Chinese eval score: F1:0.4767803739152121, EM:0.3392857142857143\n",
            "collect data:  0.6492661636739393 0.5331010452961672 0.4767803739152121 0.3392857142857143 2649 1\n",
            "Epoch: 1/3, batch: 1200/1749, 68.610635%\n",
            "English eval score: F1:0.6555989860027368, EM:0.5487804878048781\n",
            "Chinese eval score: F1:0.467064917054961, EM:0.35119047619047616\n",
            "collect data:  0.6555989860027368 0.5487804878048781 0.467064917054961 0.35119047619047616 2949 1\n",
            "Epoch: 1/3, batch: 1500/1749, 85.763293%\n",
            "English eval score: F1:0.6339305873314, EM:0.5252613240418118\n",
            "Chinese eval score: F1:0.4409939998697836, EM:0.31746031746031744\n",
            "collect data:  0.6339305873314 0.5252613240418118 0.4409939998697836 0.31746031746031744 3249 1\n",
            "Epoch: 1/3, batch: 1748/1749, 99.942824%\n",
            "English eval score: F1:0.6597869325561279, EM:0.5548780487804879\n",
            "Chinese eval score: F1:0.4549828789716847, EM:0.33134920634920634\n",
            "collect data:  0.6597869325561279 0.5548780487804879 0.4549828789716847 0.33134920634920634 3497 1\n",
            "Epoch: 2/3, batch: 0/1749, 0.000000%\n",
            "English eval score: F1:0.6610012158786429, EM:0.5531358885017421\n",
            "Chinese eval score: F1:0.45810460618445925, EM:0.32936507936507936\n",
            "collect data:  0.6610012158786429 0.5531358885017421 0.45810460618445925 0.32936507936507936 3498 2\n",
            "Epoch: 2/3, batch: 300/1749, 17.152659%\n",
            "English eval score: F1:0.6139175950628263, EM:0.4895470383275261\n",
            "Chinese eval score: F1:0.4573833600876794, EM:0.3253968253968254\n",
            "collect data:  0.6139175950628263 0.4895470383275261 0.4573833600876794 0.3253968253968254 3798 2\n",
            "Epoch: 2/3, batch: 600/1749, 34.305317%\n",
            "English eval score: F1:0.6478522325177672, EM:0.5331010452961672\n",
            "Chinese eval score: F1:0.4598022723330256, EM:0.3353174603174603\n",
            "collect data:  0.6478522325177672 0.5331010452961672 0.4598022723330256 0.3353174603174603 4098 2\n",
            "Epoch: 2/3, batch: 900/1749, 51.457976%\n",
            "English eval score: F1:0.6410624954881874, EM:0.5270034843205574\n",
            "Chinese eval score: F1:0.4499966183580679, EM:0.3194444444444444\n",
            "collect data:  0.6410624954881874 0.5270034843205574 0.4499966183580679 0.3194444444444444 4398 2\n",
            "Epoch: 2/3, batch: 1200/1749, 68.610635%\n",
            "English eval score: F1:0.6390946662602264, EM:0.5226480836236934\n",
            "Chinese eval score: F1:0.42271341190386585, EM:0.2996031746031746\n",
            "collect data:  0.6390946662602264 0.5226480836236934 0.42271341190386585 0.2996031746031746 4698 2\n",
            "Epoch: 2/3, batch: 1500/1749, 85.763293%\n",
            "English eval score: F1:0.6559131627377299, EM:0.5383275261324042\n",
            "Chinese eval score: F1:0.467538872958074, EM:0.3373015873015873\n",
            "collect data:  0.6559131627377299 0.5383275261324042 0.467538872958074 0.3373015873015873 4998 2\n",
            "Epoch: 2/3, batch: 1748/1749, 99.942824%\n",
            "English eval score: F1:0.6491078762194417, EM:0.5383275261324042\n",
            "Chinese eval score: F1:0.44503657779410494, EM:0.30357142857142855\n",
            "collect data:  0.6491078762194417 0.5383275261324042 0.44503657779410494 0.30357142857142855 5246 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcaiXnVaaSd5"
      },
      "source": [
        "with open('MUSE_lr_' + str(\"train_small\") + '.csv', 'w') as f:\n",
        "    f.write(\"En_F1,En_EM,Zh_F1,Zh_EM,dummy_index,epoch\\n\")\n",
        "    for line in collect:\n",
        "        sline = list(map(lambda x: str(x), line))\n",
        "        sline = ','.join(sline)\n",
        "        f.write(sline+\"\\n\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRAUfw1uRktB",
        "outputId": "4aa112c8-4eeb-4406-93f8-4487aee63d10"
      },
      "source": [
        "# 64 52 42 29"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT_MLQA.ipynb\t\t\t\t\t en-zh.txt\r\n",
            "BERT_MLQA_MUSE.ipynb\t\t\t\t fast_align\r\n",
            "BERT_MLQA_TAR-Copy1.ipynb\t\t\t muse_dict\r\n",
            "BERT_MLQA_TAR.ipynb\t\t\t\t train_dataset\r\n",
            "BERT_MLQA_TEST.ipynb\t\t\t\t train_large_dataset\r\n",
            "MLQA_V1\t\t\t\t\t\t train_zh_dataset\r\n",
            "MLQA_V1.zip\t\t\t\t\t val_dataset\r\n",
            "MUSE_dataset_preparation.ipynb\t\t\t zh-en.txt\r\n",
            "Question_Answering_with_a_Fine_Tuned_BERT.ipynb  zh_dataset\r\n",
            "build\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}