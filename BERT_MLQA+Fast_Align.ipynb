{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "BERT_MLQA+Fast_Align.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a820a3e278cb47b9834a995bb021c368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4cb3a2683f0f4ccb81fb4e978a02fee5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5e3fd168629f4beb9a0e960ba9413139",
              "IPY_MODEL_f7ff2a8ddbd64357aade60b932ee8e30"
            ]
          }
        },
        "4cb3a2683f0f4ccb81fb4e978a02fee5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e3fd168629f4beb9a0e960ba9413139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fc660cf4056f4c5c8492ae89344ec13c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e1685e14e7343d8977ed0baf4121dd9"
          }
        },
        "f7ff2a8ddbd64357aade60b932ee8e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e93e74e4f7a84c809ff596b6b1d62f46",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:50&lt;00:00, 12.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a173e05725564e829631086e890dc40f"
          }
        },
        "fc660cf4056f4c5c8492ae89344ec13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e1685e14e7343d8977ed0baf4121dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e93e74e4f7a84c809ff596b6b1d62f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a173e05725564e829631086e890dc40f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45ff537967b043b3999b4d57a0a91b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3c865b642e6b456bb7f4323fec4ff3f8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9dc306f5045448a1a12e18d86014fc1a",
              "IPY_MODEL_e66fb3102695485e8d3590641010f05a"
            ]
          }
        },
        "3c865b642e6b456bb7f4323fec4ff3f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9dc306f5045448a1a12e18d86014fc1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_52a05f5f37ae49e4ae63d76bbf7b8f2a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b688bc68d174c18a7f353152176f1dc"
          }
        },
        "e66fb3102695485e8d3590641010f05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e7335a7d60b14d4dbab9b4bf6d8613d9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 714M/714M [00:14&lt;00:00, 50.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9fb13c9f27b41588a6d733a6e18c037"
          }
        },
        "52a05f5f37ae49e4ae63d76bbf7b8f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b688bc68d174c18a7f353152176f1dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7335a7d60b14d4dbab9b4bf6d8613d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9fb13c9f27b41588a6d733a6e18c037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6bb5d8e030e043328ea1f2681c122ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8b57616ee2f549dfa42b45aad8cb6a53",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_512db6d0add24d6eacb6aa3254ffd832",
              "IPY_MODEL_62e5674bac7b47ed9c1059e931865ef6"
            ]
          }
        },
        "8b57616ee2f549dfa42b45aad8cb6a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "512db6d0add24d6eacb6aa3254ffd832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8169b1898c4e4ab0b0e1eab214211359",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70d01a4eef794e04a9585ed82035028f"
          }
        },
        "62e5674bac7b47ed9c1059e931865ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f9cff028ad3243c5963da56435c189ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 3.83MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7b06248dc86420bb5ce551e73c79f15"
          }
        },
        "8169b1898c4e4ab0b0e1eab214211359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70d01a4eef794e04a9585ed82035028f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9cff028ad3243c5963da56435c189ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7b06248dc86420bb5ce551e73c79f15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ebd24d6db65f42c6847a0ae3dec2bd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7429e05d814e4796ae54eda19d9fef32",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_215e22949db943e989228409f326a75e",
              "IPY_MODEL_c43a3bc2b6444385aec28a8c0873cce6"
            ]
          }
        },
        "7429e05d814e4796ae54eda19d9fef32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "215e22949db943e989228409f326a75e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d756f44c8dff4194affbb6441003fda7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1961828,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1961828,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b130d837f7214fd88f75588ef8662751"
          }
        },
        "c43a3bc2b6444385aec28a8c0873cce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1552540c34984ab1b8e573389f1987ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.96M/1.96M [00:00&lt;00:00, 6.94MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5bc06d04b74e4876943d4a984791a836"
          }
        },
        "d756f44c8dff4194affbb6441003fda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b130d837f7214fd88f75588ef8662751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1552540c34984ab1b8e573389f1987ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5bc06d04b74e4876943d4a984791a836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEnHef84-erm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebc8b01-24b8-4eed-ed42-b274b6c14e53"
      },
      "source": [
        "!pip install gdown\n",
        "!sudo apt-get install unzip\n",
        "!gdown https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip\n",
        "!unzip MLQA_V1.zip\n",
        "!rm MLQA_V1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Downloading...\n",
            "From: https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip\n",
            "To: /content/MLQA_V1.zip\n",
            "100% 75.7M/75.7M [00:07<00:00, 9.92MB/s]\n",
            "Archive:  MLQA_V1.zip\n",
            "   creating: MLQA_V1/\n",
            "   creating: MLQA_V1/dev/\n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-zh.json  \n",
            "   creating: MLQA_V1/test/\n",
            "  inflating: MLQA_V1/test/test-context-ar-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-zh.json  \n",
            "  inflating: MLQA_V1/LICENSE         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuaeZjghD7LD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1856f68-510f-4d3d-8084-0b0511a60f3c"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 39.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=850cc6e828f54bbc2b739839a82410cdd13fce06acc4be0dbbee155cd521cb07\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUpsNqXi9BgH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216,
          "referenced_widgets": [
            "a820a3e278cb47b9834a995bb021c368",
            "4cb3a2683f0f4ccb81fb4e978a02fee5",
            "5e3fd168629f4beb9a0e960ba9413139",
            "f7ff2a8ddbd64357aade60b932ee8e30",
            "fc660cf4056f4c5c8492ae89344ec13c",
            "8e1685e14e7343d8977ed0baf4121dd9",
            "e93e74e4f7a84c809ff596b6b1d62f46",
            "a173e05725564e829631086e890dc40f",
            "45ff537967b043b3999b4d57a0a91b8f",
            "3c865b642e6b456bb7f4323fec4ff3f8",
            "9dc306f5045448a1a12e18d86014fc1a",
            "e66fb3102695485e8d3590641010f05a",
            "52a05f5f37ae49e4ae63d76bbf7b8f2a",
            "7b688bc68d174c18a7f353152176f1dc",
            "e7335a7d60b14d4dbab9b4bf6d8613d9",
            "e9fb13c9f27b41588a6d733a6e18c037"
          ]
        },
        "outputId": "358fef07-d3b5-4f49-d210-2885a0de8553"
      },
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a820a3e278cb47b9834a995bb021c368",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45ff537967b043b3999b4d57a0a91b8f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlPZCHeE9BgN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "6bb5d8e030e043328ea1f2681c122ba9",
            "8b57616ee2f549dfa42b45aad8cb6a53",
            "512db6d0add24d6eacb6aa3254ffd832",
            "62e5674bac7b47ed9c1059e931865ef6",
            "8169b1898c4e4ab0b0e1eab214211359",
            "70d01a4eef794e04a9585ed82035028f",
            "f9cff028ad3243c5963da56435c189ed",
            "f7b06248dc86420bb5ce551e73c79f15"
          ]
        },
        "outputId": "aa03c226-3fcf-4f0a-a89a-fd46943ee73b"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bb5d8e030e043328ea1f2681c122ba9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eaJ40Gv9BgS"
      },
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mfOXQrI9BgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa63566-36a2-4cc4-d41f-96f7ddc39752"
      },
      "source": [
        "input_ids = tokenizer.encode(question, answer_text)\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
        "print(input_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The input has a total of 74 tokens.\n",
            "[101, 14962, 11299, 88588, 15107, 46291, 46935, 118, 12077, 10529, 136, 102, 46291, 46935, 118, 12077, 10124, 30181, 22185, 119, 119, 119, 10271, 10393, 10233, 118, 84480, 10111, 10151, 10266, 33627, 13971, 15851, 10108, 122, 117, 81201, 117, 10142, 169, 11339, 10108, 22405, 11517, 88588, 106, 18897, 14908, 14206, 10271, 10124, 122, 119, 11069, 32469, 117, 10380, 11419, 51511, 10271, 10114, 13574, 169, 20969, 15304, 10114, 13737, 10114, 20442, 36186, 10457, 34469, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCyU1vu59BgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28045ca-5260-4a46-c74b-7f0574a68cf3"
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its id...\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    \n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    \n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "How          14,962\n",
            "many         11,299\n",
            "parameters   88,588\n",
            "does         15,107\n",
            "BE           46,291\n",
            "##RT         46,935\n",
            "-               118\n",
            "large        12,077\n",
            "have         10,529\n",
            "?               136\n",
            "\n",
            "[SEP]           102\n",
            "\n",
            "BE           46,291\n",
            "##RT         46,935\n",
            "-               118\n",
            "large        12,077\n",
            "is           10,124\n",
            "really       30,181\n",
            "big          22,185\n",
            ".               119\n",
            ".               119\n",
            ".               119\n",
            "it           10,271\n",
            "has          10,393\n",
            "24           10,233\n",
            "-               118\n",
            "layers       84,480\n",
            "and          10,111\n",
            "an           10,151\n",
            "em           10,266\n",
            "##bed        33,627\n",
            "##ding       13,971\n",
            "size         15,851\n",
            "of           10,108\n",
            "1               122\n",
            ",               117\n",
            "024          81,201\n",
            ",               117\n",
            "for          10,142\n",
            "a               169\n",
            "total        11,339\n",
            "of           10,108\n",
            "340          22,405\n",
            "##M          11,517\n",
            "parameters   88,588\n",
            "!               106\n",
            "Alto         18,897\n",
            "##get        14,908\n",
            "##her        14,206\n",
            "it           10,271\n",
            "is           10,124\n",
            "1               122\n",
            ".               119\n",
            "34           11,069\n",
            "##GB         32,469\n",
            ",               117\n",
            "so           10,380\n",
            "ex           11,419\n",
            "##pect       51,511\n",
            "it           10,271\n",
            "to           10,114\n",
            "take         13,574\n",
            "a               169\n",
            "couple       20,969\n",
            "minutes      15,304\n",
            "to           10,114\n",
            "download     13,737\n",
            "to           10,114\n",
            "your         20,442\n",
            "Cola         36,186\n",
            "##b          10,457\n",
            "instance     34,469\n",
            ".               119\n",
            "\n",
            "[SEP]           102\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiUMWROF9Bgb"
      },
      "source": [
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "# The number of segment A tokens includes the [SEP] token istelf.\n",
        "num_seg_a = sep_index + 1\n",
        "\n",
        "# The remainder are segment B.\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "# Construct the list of 0s and 1s.\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "# There should be a segment_id for every input token.\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3tIwNTg9Bgd"
      },
      "source": [
        "start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP4hRQkT9Bgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eea1e3b-34b9-4069-da80-41b9db2bfa29"
      },
      "source": [
        "answer_start = torch.argmax(start_scores[0, num_seg_a:]) + num_seg_a\n",
        "answer_end = torch.argmax(end_scores[0, answer_start:]) + answer_start\n",
        "\n",
        "print(answer_start, answer_end)\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(57) tensor(64)\n",
            "Answer: \"ex ##pect it to take a couple minutes\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLp37zPz9Bgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd478928-bf9d-4b70-eb7e-cf7c15bbd0d4"
      },
      "source": [
        "# Start with the first token.\n",
        "answer = tokens[answer_start]\n",
        "\n",
        "# Select the remaining answer tokens and join them with whitespace.\n",
        "for i in range(answer_start + 1, answer_end + 1):\n",
        "    \n",
        "    # If it's a subword token, then recombine it with the previous token.\n",
        "    if tokens[i][0:2] == '##':\n",
        "        answer += tokens[i][2:]\n",
        "    \n",
        "    # Otherwise, add a space then the token.\n",
        "    else:\n",
        "        answer += ' ' + tokens[i]\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answer: \"expect it to take a couple minutes\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zXDQCOw9Bgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6371216f-6b25-4cde-dc62-2937c0f48206"
      },
      "source": [
        "import json\n",
        "def read_MLQA(path):\n",
        "    with open(path, 'r') as f:\n",
        "        MLQA_dict = json.load(f)\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for group in MLQA_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "val_contexts, val_questions, val_answers = read_MLQA('MLQA_V1/dev/dev-context-en-question-en.json')\n",
        "zh_contexts, zh_questions, zh_answers = read_MLQA('MLQA_V1/dev/dev-context-zh-question-zh.json')\n",
        "train_contexts, train_questions, train_answers = read_MLQA('MLQA_V1/test/test-context-en-question-en.json')\n",
        "print(len(train_contexts))\n",
        "print(len(val_contexts))\n",
        "print(len(zh_contexts))\n",
        "print(val_questions[0], val_answers[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11590\n",
            "1148\n",
            "504\n",
            "Does an infection for Sandflies go away over time? {'text': 'remains infected for its lifetime', 'answer_start': 571}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt7Jgm4Y9Bgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47462c4a-cf12-44fd-a2b1-ee85d053f60a"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # sometimes squad answers are off by a character or two – fix this\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "        else:\n",
        "            print('****')\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)\n",
        "add_end_idx(zh_answers, zh_contexts)\n",
        "print(val_questions[0], val_answers[0])\n",
        "print(zh_questions[2], zh_answers[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Does an infection for Sandflies go away over time? {'text': 'remains infected for its lifetime', 'answer_start': 571, 'answer_end': 604}\n",
            "俄罗斯有多少队获得参赛资格？ {'text': '十支', 'answer_start': 153, 'answer_end': 155}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGWJ35lo9Bgl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "ebd24d6db65f42c6847a0ae3dec2bd16",
            "7429e05d814e4796ae54eda19d9fef32",
            "215e22949db943e989228409f326a75e",
            "c43a3bc2b6444385aec28a8c0873cce6",
            "d756f44c8dff4194affbb6441003fda7",
            "b130d837f7214fd88f75588ef8662751",
            "1552540c34984ab1b8e573389f1987ef",
            "5bc06d04b74e4876943d4a984791a836"
          ]
        },
        "outputId": "0a5c5c13-30a2-432e-8bfa-6bde9c2a79e4"
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
        "zh_encodings = tokenizer(zh_contexts, zh_questions, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebd24d6db65f42c6847a0ae3dec2bd16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961828.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmMHghk49Bgn"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "        # if None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)\n",
        "add_token_positions(zh_encodings, zh_answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIiBFY5W9Bgo"
      },
      "source": [
        "import torch\n",
        "\n",
        "class MLQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = MLQADataset(train_encodings)\n",
        "val_dataset = MLQADataset(val_encodings)\n",
        "zh_dataset = MLQADataset(zh_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2tW0HGR9Bgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b004e11f-c63a-4b61-8fc9-a5e508e44abe"
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCfT9uDU9Bgr"
      },
      "source": [
        "def compute_f1(predicted, true):\n",
        "    c = len(set(predicted) & set(true))\n",
        "    l1 = len(predicted)\n",
        "    l2 = len(true)\n",
        "    if(l1 + l2 == 0):\n",
        "        return 1\n",
        "    f1 = 2*c/(l1+l2)\n",
        "    return f1\n",
        "    \n",
        "def compute_em(predicted, true):\n",
        "    return int(predicted == true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PeTvnSXA9Bgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a063f80-cebe-4041-da6e-cb3db6ef6ee4"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "\n",
        "# input_ids = None\n",
        "# attention_mask = None\n",
        "# start_positions = None\n",
        "# end_positions = None\n",
        "# model = None\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "zh_loader = DataLoader(zh_dataset, batch_size=8, shuffle=True)\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "optim = AdamW(model.parameters(), lr=5e-6) #1e-5 #10e-5 #5e-6\n",
        "\n",
        "\n",
        "val_batch = 300\n",
        "max_epoch = 3\n",
        "train_batch = len(train_loader)\n",
        "for epoch in range(max_epoch):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        if batch_idx % val_batch == 0 or batch_idx == train_batch - 1:\n",
        "            print(\"Epoch: {}/{}, batch: {}/{}, {:%}\".format(epoch, max_epoch, batch_idx, train_batch, batch_idx/train_batch))\n",
        "            model.eval()\n",
        "            eval_cnt = 0\n",
        "            F1 = 0.0\n",
        "            EM = 0.0\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                optim.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "                samples_in_batch = len(input_ids)\n",
        "                for i in range(samples_in_batch):\n",
        "                    predict_start = int(outputs[1][i].argmax().cpu())\n",
        "                    predict_end = int(outputs[2][i].argmax().cpu())\n",
        "                    true_start = int(start_positions[i].cpu())\n",
        "                    true_end = int(end_positions[i].cpu())\n",
        "                    F1 += compute_f1(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                    EM += compute_em(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                eval_cnt += samples_in_batch\n",
        "            F1 /= eval_cnt\n",
        "            EM /= eval_cnt\n",
        "            print(\"English eval score: F1:{}, EM:{}\".format(F1, EM))\n",
        "            \n",
        "            eval_cnt = 0\n",
        "            F1 = 0.0\n",
        "            EM = 0.0\n",
        "            for batch_idx, batch in enumerate(zh_loader):\n",
        "                optim.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "                samples_in_batch = len(input_ids)\n",
        "                for i in range(samples_in_batch):\n",
        "                    predict_start = int(outputs[1][i].argmax().cpu())\n",
        "                    predict_end = int(outputs[2][i].argmax().cpu())\n",
        "                    true_start = int(start_positions[i].cpu())\n",
        "                    true_end = int(end_positions[i].cpu())\n",
        "                    F1 += compute_f1(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                    EM += compute_em(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                eval_cnt += samples_in_batch\n",
        "            F1 /= eval_cnt\n",
        "            EM /= eval_cnt\n",
        "            print(\"Chinese eval score: F1:{}, EM:{}\".format(F1, EM))\n",
        "            \n",
        "        \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1449\n",
            "144\n",
            "Epoch: 0/9, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.14132579769905074, EM:0.12195121951219512\n",
            "Chinese eval score: F1:0.08902808242330476, EM:0.05555555555555555\n",
            "Epoch: 0/9, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.5238848550113626, EM:0.42770034843205573\n",
            "Chinese eval score: F1:0.3574842652658238, EM:0.23809523809523808\n",
            "Epoch: 0/9, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.5778699044876141, EM:0.46689895470383275\n",
            "Chinese eval score: F1:0.36082471078871003, EM:0.2361111111111111\n",
            "Epoch: 0/9, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.5970574622237192, EM:0.490418118466899\n",
            "Chinese eval score: F1:0.39859522061238595, EM:0.2777777777777778\n",
            "Epoch: 0/9, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.6163143067286825, EM:0.5026132404181185\n",
            "Chinese eval score: F1:0.41574455581681036, EM:0.2757936507936508\n",
            "Epoch: 0/9, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.6413038219338919, EM:0.519163763066202\n",
            "Chinese eval score: F1:0.44426846568071526, EM:0.31547619047619047\n",
            "Epoch: 1/9, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.640749101465551, EM:0.5182926829268293\n",
            "Chinese eval score: F1:0.44557647078588475, EM:0.31547619047619047\n",
            "Epoch: 1/9, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.6301158365874548, EM:0.5139372822299652\n",
            "Chinese eval score: F1:0.43049813741769605, EM:0.30158730158730157\n",
            "Epoch: 1/9, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.63615163118339, EM:0.5200348432055749\n",
            "Chinese eval score: F1:0.41296034228718287, EM:0.28174603174603174\n",
            "Epoch: 1/9, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.6492104071077152, EM:0.5357142857142857\n",
            "Chinese eval score: F1:0.43047477893795344, EM:0.2976190476190476\n",
            "Epoch: 1/9, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.6333197976292527, EM:0.5313588850174216\n",
            "Chinese eval score: F1:0.41513250358127607, EM:0.27976190476190477\n",
            "Epoch: 1/9, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.6457181578227011, EM:0.5331010452961672\n",
            "Chinese eval score: F1:0.40663828031339794, EM:0.27380952380952384\n",
            "Epoch: 2/9, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.6409833052189275, EM:0.5296167247386759\n",
            "Chinese eval score: F1:0.3979312443066713, EM:0.27380952380952384\n",
            "Epoch: 2/9, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.630041112087838, EM:0.5165505226480837\n",
            "Chinese eval score: F1:0.40406363167090964, EM:0.2916666666666667\n",
            "Epoch: 2/9, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.6478425400248837, EM:0.539198606271777\n",
            "Chinese eval score: F1:0.4025597318649118, EM:0.27380952380952384\n",
            "Epoch: 2/9, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.6420669355239924, EM:0.5365853658536586\n",
            "Chinese eval score: F1:0.43619900660182, EM:0.2837301587301587\n",
            "Epoch: 2/9, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.6274447342576824, EM:0.5165505226480837\n",
            "Chinese eval score: F1:0.42689078968847205, EM:0.29563492063492064\n",
            "Epoch: 2/9, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.6517374058792108, EM:0.5487804878048781\n",
            "Chinese eval score: F1:0.40801426652923, EM:0.28174603174603174\n",
            "Epoch: 3/9, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.6534643758480612, EM:0.5513937282229965\n",
            "Chinese eval score: F1:0.4012621294317297, EM:0.28174603174603174\n",
            "Epoch: 3/9, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.6330220978013661, EM:0.5313588850174216\n",
            "Chinese eval score: F1:0.41137700466290095, EM:0.29563492063492064\n",
            "Epoch: 3/9, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.5956481814049923, EM:0.4886759581881533\n",
            "Chinese eval score: F1:0.39394562368825975, EM:0.26785714285714285\n",
            "Epoch: 3/9, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.6316394852559896, EM:0.5383275261324042\n",
            "Chinese eval score: F1:0.41634496409406263, EM:0.2857142857142857\n",
            "Epoch: 3/9, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.6372566298374513, EM:0.5339721254355401\n",
            "Chinese eval score: F1:0.4238876794300356, EM:0.30158730158730157\n",
            "Epoch: 3/9, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.65115587903237, EM:0.5383275261324042\n",
            "Chinese eval score: F1:0.40183784362619085, EM:0.2857142857142857\n",
            "Epoch: 4/9, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.6477869620947927, EM:0.5365853658536586\n",
            "Chinese eval score: F1:0.40901496647146407, EM:0.29563492063492064\n",
            "Epoch: 4/9, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.6151636226053072, EM:0.5052264808362369\n",
            "Chinese eval score: F1:0.40860207689454425, EM:0.28174603174603174\n",
            "Epoch: 4/9, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.6292355953644627, EM:0.5139372822299652\n",
            "Chinese eval score: F1:0.4050766609484739, EM:0.28174603174603174\n",
            "Epoch: 4/9, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.6365477749909233, EM:0.524390243902439\n",
            "Chinese eval score: F1:0.4227878090942181, EM:0.2777777777777778\n",
            "Epoch: 4/9, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.620272218850749, EM:0.509581881533101\n",
            "Chinese eval score: F1:0.44233434484918405, EM:0.29563492063492064\n",
            "Epoch: 4/9, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.6349698006447296, EM:0.5209059233449478\n",
            "Chinese eval score: F1:0.3925858031569591, EM:0.2638888888888889\n",
            "Epoch: 5/9, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.6332255551736926, EM:0.5200348432055749\n",
            "Chinese eval score: F1:0.3977779667924722, EM:0.26587301587301587\n",
            "Epoch: 5/9, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.6221571661103555, EM:0.5052264808362369\n",
            "Chinese eval score: F1:0.4076863097028814, EM:0.26785714285714285\n",
            "Epoch: 5/9, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.6480360835981963, EM:0.5365853658536586\n",
            "Chinese eval score: F1:0.4254902881627376, EM:0.2876984126984127\n",
            "Epoch: 5/9, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.6233352363032684, EM:0.5121951219512195\n",
            "Chinese eval score: F1:0.38072811619078345, EM:0.25396825396825395\n",
            "Epoch: 5/9, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.6158447500875525, EM:0.5043554006968641\n",
            "Chinese eval score: F1:0.4133198488322964, EM:0.2777777777777778\n",
            "Epoch: 5/9, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.6218191888717588, EM:0.5209059233449478\n",
            "Chinese eval score: F1:0.3868670927564877, EM:0.2757936507936508\n",
            "Epoch: 6/9, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.6243389557914811, EM:0.5217770034843205\n",
            "Chinese eval score: F1:0.38037487646921386, EM:0.26785714285714285\n",
            "Epoch: 6/9, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.6163513800386612, EM:0.5008710801393729\n",
            "Chinese eval score: F1:0.41258706533274203, EM:0.27976190476190477\n",
            "Epoch: 6/9, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.6192579510640066, EM:0.5156794425087108\n",
            "Chinese eval score: F1:0.4069621830897531, EM:0.2718253968253968\n",
            "Epoch: 6/9, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.6456569704259522, EM:0.5217770034843205\n",
            "Chinese eval score: F1:0.3623364286080932, EM:0.2261904761904762\n",
            "Epoch: 6/9, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.6341909154692229, EM:0.5139372822299652\n",
            "Chinese eval score: F1:0.400926779832774, EM:0.2718253968253968\n",
            "Epoch: 6/9, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.6201288010434463, EM:0.5121951219512195\n",
            "Chinese eval score: F1:0.42431769890552495, EM:0.2857142857142857\n",
            "Epoch: 7/9, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.6173312338915228, EM:0.509581881533101\n",
            "Chinese eval score: F1:0.42088327026472283, EM:0.2837301587301587\n",
            "Epoch: 7/9, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.6185787233272595, EM:0.509581881533101\n",
            "Chinese eval score: F1:0.38841134578243064, EM:0.25396825396825395\n",
            "Epoch: 7/9, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.6136433201765269, EM:0.5069686411149826\n",
            "Chinese eval score: F1:0.39214802273523985, EM:0.26785714285714285\n",
            "Epoch: 7/9, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.6175757452960171, EM:0.5130662020905923\n",
            "Chinese eval score: F1:0.35557491138326264, EM:0.2400793650793651\n",
            "Epoch: 7/9, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.63972687899675, EM:0.5383275261324042\n",
            "Chinese eval score: F1:0.4178827277889076, EM:0.29563492063492064\n",
            "Epoch: 7/9, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.6155054255784732, EM:0.5121951219512195\n",
            "Chinese eval score: F1:0.3781525436183793, EM:0.25793650793650796\n",
            "Epoch: 8/9, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.6105762206118842, EM:0.5069686411149826\n",
            "Chinese eval score: F1:0.37421556127915073, EM:0.251984126984127\n",
            "Epoch: 8/9, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.6293348530572948, EM:0.5226480836236934\n",
            "Chinese eval score: F1:0.4056912291983417, EM:0.28174603174603174\n",
            "Epoch: 8/9, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.6181816940003607, EM:0.5121951219512195\n",
            "Chinese eval score: F1:0.3728243740336821, EM:0.25992063492063494\n",
            "Epoch: 8/9, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.6274318002840218, EM:0.5156794425087108\n",
            "Chinese eval score: F1:0.398774251085705, EM:0.25793650793650796\n",
            "Epoch: 8/9, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.6218152880063067, EM:0.5104529616724739\n",
            "Chinese eval score: F1:0.4315881173768248, EM:0.2896825396825397\n",
            "Epoch: 8/9, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.6340753276580648, EM:0.5156794425087108\n",
            "Chinese eval score: F1:0.37662566820546617, EM:0.2619047619047619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_t38ywgE9Bgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82231c6a-ffa6-43cb-b780-ea3f6153d971"
      },
      "source": [
        "# FOR TESTING ONLY IGNORE\n",
        "import json\n",
        "with open(\"MLQA_V1/dev/dev-context-en-question-en.json\",'r') as load_f:\n",
        "    load_dict = json.load(load_f)\n",
        "    print(load_dict.keys())\n",
        "    print(len(load_dict['data']))\n",
        "    print(load_dict['data'][0].keys())\n",
        "    print(load_dict['data'][0]['title'])\n",
        "    print(load_dict['data'][0]['paragraphs'])\n",
        "    print(load_dict['data'][0]['paragraphs'][0].keys())\n",
        "#     print(load_dict['data'][0]['paragraphs'][0]['context'])\n",
        "    print(load_dict['data'][0]['paragraphs'][0]['qas'])\n",
        "    print(load_dict['data'][0]['paragraphs'][0]['qas'][0])\n",
        "    \n",
        "#TODO:(about code)\n",
        "# 1. baseline Evaluation metric, Train faster (finished)\n",
        "# 1.5 need to compare eng and chinese\n",
        "# 2. word-level translate and generate new dataset of another language\n",
        "# 3. sentence-level translate and alignment + retrive(min, max as tar) \n",
        "\n",
        "# 可能 \n",
        "# 输入：英文句子+中文句子（乱序） \n",
        "# 输出：单词的对应关系\n",
        "\n",
        "# English eval score: F1:0.6419080228993544, EM:0.5287456445993032\n",
        "# Chinese eval score: F1:0.42465514513374364, EM:0.2996031746031746"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['version', 'data'])\n",
            "520\n",
            "dict_keys(['title', 'paragraphs'])\n",
            "Pappataci fever\n",
            "[{'context': 'Pappataci fever is prevalent in the subtropical zone of the Eastern Hemisphere between 20°N and 45°N, particularly in Southern Europe, North Africa, the Balkans, Eastern Mediterranean, Iraq, Iran, Pakistan, Afghanistan and India.The disease is transmitted by the bites of phlebotomine sandflies of the Genus Phlebotomus, in particular, Phlebotomus papatasi, Phlebotomus perniciosus and Phlebotomus perfiliewi. The sandfly becomes infected when biting an infected human in the period between 48 hours before the onset of fever and 24 hours after the end of the fever, and remains infected for its lifetime. Besides this horizontal virus transmission from man to sandfly, the virus can be transmitted in insects transovarially, from an infected female sandfly to its offspring.Pappataci fever is seldom recognised in endemic populations because it is mixed with other febrile illnesses of childhood, but it is more well-known among immigrants and military personnel from non-endemic regions.', 'qas': [{'question': 'Does an infection for Sandflies go away over time?', 'answers': [{'text': 'remains infected for its lifetime', 'answer_start': 571}], 'id': '569666f4dc3983dab5624e989212c1d9d0cd1798'}]}]\n",
            "dict_keys(['context', 'qas'])\n",
            "[{'question': 'Does an infection for Sandflies go away over time?', 'answers': [{'text': 'remains infected for its lifetime', 'answer_start': 571}], 'id': '569666f4dc3983dab5624e989212c1d9d0cd1798'}]\n",
            "{'question': 'Does an infection for Sandflies go away over time?', 'answers': [{'text': 'remains infected for its lifetime', 'answer_start': 571}], 'id': '569666f4dc3983dab5624e989212c1d9d0cd1798'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVdT5rC7PwsJ"
      },
      "source": [
        "Fast Align"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JusOW4U2EXNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c58cfd-02ee-45a4-d09c-c3eb243790aa"
      },
      "source": [
        "! git clone https://github.com/clab/fast_align.git\n",
        "! sudo apt - get install libgoogle - perftools - dev libsparsehash - dev\n",
        "% cd fast_align\n",
        "! mkdir build\n",
        "% cd build\n",
        "! cmake ..\n",
        "! make"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fast_align'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 213 (delta 2), reused 4 (delta 2), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (213/213), 70.68 KiB | 5.05 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "E: Invalid operation get\n",
            "/content/fast_align\n",
            "/content/fast_align/build\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/fast_align/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target atools\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/alignment_io.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/atools.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable atools\u001b[0m\n",
            "[ 50%] Built target atools\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fast_align\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/fast_align.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/ttables.cc.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable fast_align\u001b[0m\n",
            "[100%] Built target fast_align\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fv0qsfy5ZKi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb5a9d2b-415a-4ac2-dc3b-2fe19bcb4ec0"
      },
      "source": [
        "% pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/fast_align/build'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGhihkXOP23v"
      },
      "source": [
        "! echo \"doch jetzt ist der Held gefallen . ||| but now the hero has fallen .\" >> sample.txt\n",
        "! echo \"neue Modelle werden erprobt . ||| new models are being tested .\" >> sample.txt\n",
        "! echo \"doch fehlen uns neue Ressourcen . ||| but we lack new resources .\" >> sample.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFukFUOw5Kb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5577d0-38fe-4d3a-9330-4bb6f96f5828"
      },
      "source": [
        "! ./fast_align -i sample.txt -d -o -v > forward.align"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ARG=i\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=v\n",
            "INITIAL PASS \n",
            "expected target length = source length * 1.06667\n",
            "ITERATION 1\n",
            "  log_e likelihood: -393.742\n",
            "  log_2 likelihood: -568.05\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.205882\n",
            "       size counts: 3\n",
            "ITERATION 2\n",
            "  log_e likelihood: -55.5823\n",
            "  log_2 likelihood: -80.1883\n",
            "     cross entropy: 4.22044\n",
            "        perplexity: 18.6414\n",
            "      posterior p0: 0.00102666\n",
            " posterior al-feat: -0.159462\n",
            "       size counts: 3\n",
            "  1  model al-feat: -0.150966 (tension=4)\n",
            "  2  model al-feat: -0.155505 (tension=3.83009)\n",
            "  3  model al-feat: -0.157673 (tension=3.75095)\n",
            "  4  model al-feat: -0.158664 (tension=3.71517)\n",
            "  5  model al-feat: -0.159109 (tension=3.69923)\n",
            "  6  model al-feat: -0.159306 (tension=3.69216)\n",
            "  7  model al-feat: -0.159393 (tension=3.68904)\n",
            "  8  model al-feat: -0.159431 (tension=3.68767)\n",
            "     final tension: 3.68706\n",
            "ITERATION 3\n",
            "  log_e likelihood: -25.4897\n",
            "  log_2 likelihood: -36.7738\n",
            "     cross entropy: 1.93547\n",
            "        perplexity: 3.82501\n",
            "      posterior p0: 4.85937e-17\n",
            " posterior al-feat: -0.158471\n",
            "       size counts: 3\n",
            "  1  model al-feat: -0.159448 (tension=3.68706)\n",
            "  2  model al-feat: -0.158903 (tension=3.70661)\n",
            "  3  model al-feat: -0.158663 (tension=3.71524)\n",
            "  4  model al-feat: -0.158556 (tension=3.71907)\n",
            "  5  model al-feat: -0.158509 (tension=3.72077)\n",
            "  6  model al-feat: -0.158488 (tension=3.72152)\n",
            "  7  model al-feat: -0.158479 (tension=3.72186)\n",
            "  8  model al-feat: -0.158474 (tension=3.72201)\n",
            "     final tension: 3.72207\n",
            "ITERATION 4\n",
            "  log_e likelihood: -23.551\n",
            "  log_2 likelihood: -33.9769\n",
            "     cross entropy: 1.78826\n",
            "        perplexity: 3.45398\n",
            "      posterior p0: 7.29047e-42\n",
            " posterior al-feat: -0.158437\n",
            "       size counts: 3\n",
            "  1  model al-feat: -0.158473 (tension=3.72207)\n",
            "  2  model al-feat: -0.158453 (tension=3.72278)\n",
            "  3  model al-feat: -0.158444 (tension=3.7231)\n",
            "  4  model al-feat: -0.15844 (tension=3.72324)\n",
            "  5  model al-feat: -0.158438 (tension=3.7233)\n",
            "  6  model al-feat: -0.158438 (tension=3.72333)\n",
            "  7  model al-feat: -0.158437 (tension=3.72334)\n",
            "  8  model al-feat: -0.158437 (tension=3.72335)\n",
            "     final tension: 3.72335\n",
            "ITERATION 5 (FINAL)\n",
            "  log_e likelihood: -23.4877\n",
            "  log_2 likelihood: -33.8855\n",
            "     cross entropy: 1.78345\n",
            "        perplexity: 3.44248\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x2u26h_P82r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d17204a2-0a8e-4971-abeb-f3a8709cad12"
      },
      "source": [
        "! cat forward.align"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0-0 1-1 2-2 3-3 4-4 5-5 6-6\n",
            "0-0 1-1 2-2 2-3 3-4 4-5\n",
            "0-0 1-1 2-2 3-3 4-4 5-5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}