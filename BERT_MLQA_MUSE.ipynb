{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT_MLQA_MUSE.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b37dbfc6d474ce2ade79daefad3db82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9560a6b58f3141b6b1da46145aa13914",
              "IPY_MODEL_0e94ea45f50140c6b0cd74452a596ea4"
            ],
            "layout": "IPY_MODEL_fd2dfe8dae7949759d3cd386077bac03"
          }
        },
        "9560a6b58f3141b6b1da46145aa13914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a426a8fc55b44e58f6ec71e4d4d7322",
            "max": 995526,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82cc8f7cfa83454ebca3d6d3e269ab4d",
            "value": 995526
          }
        },
        "0e94ea45f50140c6b0cd74452a596ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a0d66a2c94f4592b4a88d838447985b",
            "placeholder": "​",
            "style": "IPY_MODEL_149b51c4593848e09f58ed4a27af399d",
            "value": " 996k/996k [00:02&lt;00:00, 454kB/s]"
          }
        },
        "fd2dfe8dae7949759d3cd386077bac03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a426a8fc55b44e58f6ec71e4d4d7322": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82cc8f7cfa83454ebca3d6d3e269ab4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "8a0d66a2c94f4592b4a88d838447985b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "149b51c4593848e09f58ed4a27af399d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d7703029a7f4dc0a238dfefa30268a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ee93d74658744329130f19ed059a8ae",
              "IPY_MODEL_11b7d9cb014049be92777cf799a59a98"
            ],
            "layout": "IPY_MODEL_5af095746f954176bda7bbecc7e588d5"
          }
        },
        "9ee93d74658744329130f19ed059a8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5faa6ab585a4c94b3580cf538900312",
            "max": 1961828,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7be90c83c78648f28c48215d4045225b",
            "value": 1961828
          }
        },
        "11b7d9cb014049be92777cf799a59a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d698036c0b3e429caa5b9e1125074226",
            "placeholder": "​",
            "style": "IPY_MODEL_92bc7ffa8e82496abda189f8550b43fa",
            "value": " 1.96M/1.96M [00:00&lt;00:00, 2.44MB/s]"
          }
        },
        "5af095746f954176bda7bbecc7e588d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5faa6ab585a4c94b3580cf538900312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7be90c83c78648f28c48215d4045225b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "d698036c0b3e429caa5b9e1125074226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92bc7ffa8e82496abda189f8550b43fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdCYnFnuDswQ",
        "outputId": "36190b76-cf77-4903-91f4-995779f80601"
      },
      "source": [
        "! pip install opencc\n",
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/b4/24e677e135df130fc6989929dc3990a1ae19948daf28beb8f910b4f7b671/OpenCC-1.1.1.post1-py2.py3-none-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 6.0MB/s \n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 5.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 29.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f3acac19fb860e6a816f0e9e397ea045cadc1f667114317c188352530a066066\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MK3QP6qE-gz",
        "outputId": "de7343dc-3ac0-4f90-80c9-d9a3463e9979"
      },
      "source": [
        "!pip install gdown\n",
        "!sudo apt-get install unzip\n",
        "!gdown https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip\n",
        "!unzip MLQA_V1.zip\n",
        "!rm MLQA_V1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Downloading...\n",
            "From: https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip\n",
            "To: /content/MLQA_V1.zip\n",
            "100% 75.7M/75.7M [00:07<00:00, 9.55MB/s]\n",
            "Archive:  MLQA_V1.zip\n",
            "   creating: MLQA_V1/\n",
            "   creating: MLQA_V1/dev/\n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-zh.json  \n",
            "   creating: MLQA_V1/test/\n",
            "  inflating: MLQA_V1/test/test-context-ar-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-zh.json  \n",
            "  inflating: MLQA_V1/LICENSE         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GttYVun4DYi5",
        "outputId": "39d35a8e-4282-4d8a-f0f7-55fda89b9161"
      },
      "source": [
        "# MUSE\n",
        "import opencc\n",
        "converter = opencc.OpenCC('t2s.json')\n",
        "print(converter.convert('漢字'))  # 漢字\n",
        "\n",
        "convert_dict={}\n",
        "with open('en-zh.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        en, zh = line.strip().split()\n",
        "        if en not in convert_dict:\n",
        "            convert_dict[en] = converter.convert(zh)\n",
        "# print(len(convert_dict))\n",
        "            \n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-en-ro\")\n",
        "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\")\n",
        "article = \"UN Chief Says There Is No Military Solution in Syria\"\n",
        "batch = tokenizer.prepare_seq2seq_batch(src_texts=[article], src_lang=\"en_XX\")\n",
        "translated_tokens = model.generate(**batch, decoder_start_token_id=tokenizer.lang_code_to_id[\"ro_RO\"])\n",
        "translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def convert_a_line(s):\n",
        "    ret = ''\n",
        "    for word in word_tokenize(s.strip()):\n",
        "        t = word.lower()\n",
        "        if t in stop_words:\n",
        "            continue\n",
        "#         ret += t\n",
        "        st = stemmer.stem(t)\n",
        "        if t in convert_dict:\n",
        "            ret += convert_dict[word.lower()]\n",
        "        elif st in convert_dict:\n",
        "            ret += convert_dict[st]\n",
        "        elif len(st) > 5 and st + 'e' in convert_dict:\n",
        "            ret += convert_dict[st + 'e']\n",
        "        else:\n",
        "#             ret += word\n",
        "            ret += '[UNK]'\n",
        "#             ret += ''\n",
        "#             print(word)\n",
        "    return ret\n",
        "\n",
        "        \n",
        "\n",
        "def read_MLQA_to_chinese(path):\n",
        "    with open(path, 'r') as f:\n",
        "        MLQA_dict = json.load(f)\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for group in MLQA_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    before_answer = context[:answer['answer_start']]\n",
        "                    after_answer = context[answer['answer_start']:]\n",
        "                    c_before_answer = convert_a_line(before_answer)\n",
        "                    c_after_answer = convert_a_line(after_answer)\n",
        "                    c = c_before_answer + c_after_answer\n",
        "                    q = convert_a_line(question)\n",
        "                    a = {'text': convert_a_line(answer['text']), 'answer_start': len(c_before_answer)}\n",
        "                    if '从业人员利用' in c:\n",
        "                        print('****')\n",
        "                        print(c)\n",
        "                        print(q)\n",
        "                        print(a)\n",
        "                    contexts.append(c)\n",
        "                    questions.append(q)\n",
        "                    answers.append(a)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "train_contexts_muse, train_questions_muse, train_answers_muse = read_MLQA_to_chinese('MLQA_V1/test/test-context-en-question-en.json')\n",
        "print(train_contexts_muse[12])\n",
        "print(train_questions_muse[12])\n",
        "print(train_answers_muse[12])\n",
        "print(len(train_contexts_muse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "汉字\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "****\n",
            "从业人员利用[UNK]温暖区域子午线点数意图刺激环流点数诱导[UNK]流血液[UNK][UNK][UNK]对待条件[UNK][UNK]冷冷[UNK][UNK]杨缺陷[UNK]中文药品[UNK]认领[UNK][UNK]冷冷潮湿尸体[UNK]服务转[UNK][UNK]索赔[UNK]尤其有效治疗慢性问题[UNK][UNK][UNK]条件[UNK][UNK]弱[UNK][UNK][UNK][UNK]扁que[UNK][UNK][UNK][UNK][UNK]bce[UNK][UNK]一著名[UNK]医师中文古物首先专家[UNK][UNK]讨论福利[UNK]针灸经典工作扁que[UNK][UNK]断言[UNK][UNK]加上新能量尸体[UNK]对待过剩[UNK]条件[UNK]\n",
            "使用温暖区域[UNK]\n",
            "{'text': '[UNK]', 'answer_start': 6}\n",
            "原文矩形基地[UNK][UNK]万里[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]新郎箱[UNK][UNK]矩形面积测量[UNK][UNK]万里[UNK][UNK][UNK][UNK][UNK][UNK]受限领空[UNK]面积连线内部[UNK]测试网站[UNK]nts[UNK]公路网路[UNK][UNK]公路领先南水星西边[UNK]平面[UNK]领先东北湖泊[UNK]广[UNK]新郎湖泊公路跑传球[UNK]丘陵[UNK]公路[UNK]led地雷新郎盆地[UNK]改进sinc[UNK][UNK]发条课程跑过往[UNK]关卡[UNK]受限面积围绕著基地延伸[UNK]东[UNK][UNK]受限面积[UNK]新郎湖泊公路[UNK][UNK]下限[UNK]河谷[UNK]传球[UNK]入场[UNK]小型牧场[UNK][UNK]国家路线[UNK][UNK][UNK][UNK]高速公路[UNK][UNK]南rachel[UNK]\n",
            "新郎湖泊公路头亲戚湖泊[UNK]\n",
            "{'text': '东北', 'answer_start': 193}\n",
            "11590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5cbsJ9fDYi5",
        "outputId": "7ce40c97-975d-4fa2-abf0-0cd0f0ac493f"
      },
      "source": [
        "import json\n",
        "def read_MLQA(path):\n",
        "    with open(path, 'r') as f:\n",
        "        MLQA_dict = json.load(f)\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for group in MLQA_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "val_contexts, val_questions, val_answers = read_MLQA('MLQA_V1/dev/dev-context-en-question-en.json')\n",
        "zh_contexts, zh_questions, zh_answers = read_MLQA('MLQA_V1/dev/dev-context-zh-question-zh.json')\n",
        "train_contexts, train_questions, train_answers = read_MLQA('MLQA_V1/test/test-context-en-question-en.json')\n",
        "\n",
        "\n",
        "print(len(train_contexts))\n",
        "print(len(val_contexts))\n",
        "print(len(zh_contexts))\n",
        "print(val_questions[0], val_answers[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11590\n",
            "1148\n",
            "504\n",
            "Does an infection for Sandflies go away over time? {'text': 'remains infected for its lifetime', 'answer_start': 571}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g9I6d8ZDYi5"
      },
      "source": [
        "use_original_p = 0.8 # = 1-use_muse\n",
        "use_original = int(use_original_p * len(train_contexts))\n",
        "\n",
        "train_contexts[use_original:] = train_contexts_muse[use_original:]\n",
        "train_questions[use_original:] = train_questions_muse[use_original:]\n",
        "train_answers[use_original:] = train_answers_muse[use_original:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhnPHBFNDYi5",
        "outputId": "49635af7-e766-4d35-8810-aa756f856aa8"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # sometimes squad answers are off by a character or two – fix this\n",
        "        if context[start_idx:end_idx].lower() == gold_text.lower():\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "        else:\n",
        "#             print(context[start_idx:end_idx])\n",
        "#             print(gold_text)\n",
        "#             print(context[start_idx:end_idx] == gold_text)\n",
        "#             print(context)\n",
        "#             print(start_idx)\n",
        "            answer['answer_end'] = end_idx\n",
        "            print('***')\n",
        "#             jkalsdjkl\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)\n",
        "add_end_idx(zh_answers, zh_contexts)\n",
        "print(val_questions[0], val_answers[0])\n",
        "print(zh_questions[2], zh_answers[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***\n",
            "***\n",
            "***\n",
            "***\n",
            "***\n",
            "***\n",
            "***\n",
            "***\n",
            "***\n",
            "***\n",
            "***\n",
            "***\n",
            "Does an infection for Sandflies go away over time? {'text': 'remains infected for its lifetime', 'answer_start': 571, 'answer_end': 604}\n",
            "俄罗斯有多少队获得参赛资格？ {'text': '十支', 'answer_start': 153, 'answer_end': 155}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "9b37dbfc6d474ce2ade79daefad3db82",
            "9560a6b58f3141b6b1da46145aa13914",
            "0e94ea45f50140c6b0cd74452a596ea4",
            "fd2dfe8dae7949759d3cd386077bac03",
            "2a426a8fc55b44e58f6ec71e4d4d7322",
            "82cc8f7cfa83454ebca3d6d3e269ab4d",
            "8a0d66a2c94f4592b4a88d838447985b",
            "149b51c4593848e09f58ed4a27af399d",
            "3d7703029a7f4dc0a238dfefa30268a6",
            "9ee93d74658744329130f19ed059a8ae",
            "11b7d9cb014049be92777cf799a59a98",
            "5af095746f954176bda7bbecc7e588d5",
            "c5faa6ab585a4c94b3580cf538900312",
            "7be90c83c78648f28c48215d4045225b",
            "d698036c0b3e429caa5b9e1125074226",
            "92bc7ffa8e82496abda189f8550b43fa"
          ]
        },
        "id": "CkwM8flgDYi5",
        "outputId": "f485b4f1-ed42-4b5d-c7d4-3285f49d3091"
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
        "zh_encodings = tokenizer(zh_contexts, zh_questions, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b37dbfc6d474ce2ade79daefad3db82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d7703029a7f4dc0a238dfefa30268a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961828.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikfxX26GDYi5"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "#         print(answers[i]['answer_end'] - 1)\n",
        "        end_positions.append(encodings.char_to_token(i, max(answers[i]['answer_end'] - 1, 0)))\n",
        "        # if None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)\n",
        "add_token_positions(zh_encodings, zh_answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyjYU5UxDYi6"
      },
      "source": [
        "import torch\n",
        "\n",
        "class MLQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = MLQADataset(train_encodings)\n",
        "val_dataset = MLQADataset(val_encodings)\n",
        "zh_dataset = MLQADataset(zh_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shgcIO_7DYi6"
      },
      "source": [
        "def compute_f1(predicted, true):\n",
        "    c = len(set(predicted) & set(true))\n",
        "    l1 = len(predicted)\n",
        "    l2 = len(true)\n",
        "    if(l1 + l2 == 0):\n",
        "        return 1\n",
        "    f1 = 2*c/(l1+l2)\n",
        "    return f1\n",
        "    \n",
        "def compute_em(predicted, true):\n",
        "    return int(predicted == true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4iP6YQgDYi6",
        "scrolled": true,
        "outputId": "26f828f3-1e93-426b-899a-f9da8ccc41a4"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "# input_ids = None\n",
        "# attention_mask = None\n",
        "# start_positions = None\n",
        "# end_positions = None\n",
        "# model = None\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "zh_loader = DataLoader(zh_dataset, batch_size=8, shuffle=True)\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "learning_rate = 5e-6\n",
        "dummy_index = 0\n",
        "collect = []\n",
        "optim = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "val_batch = 300\n",
        "max_epoch = 3\n",
        "train_batch = len(train_loader)\n",
        "for epoch in range(max_epoch):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        if batch_idx % val_batch == 0 or batch_idx == train_batch - 1:\n",
        "            print(\"Epoch: {}/{}, batch: {}/{}, {:%}\".format(epoch, max_epoch, batch_idx, train_batch, batch_idx/train_batch))\n",
        "            dummy_index = epoch * train_batch + batch_idx\n",
        "            model.eval()\n",
        "            eval_cnt = 0\n",
        "            F1 = 0.0\n",
        "            EM = 0.0\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                optim.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "                samples_in_batch = len(input_ids)\n",
        "                for i in range(samples_in_batch):\n",
        "                    predict_start = int(outputs[1][i].argmax().cpu())\n",
        "                    predict_end = int(outputs[2][i].argmax().cpu())\n",
        "                    true_start = int(start_positions[i].cpu())\n",
        "                    true_end = int(end_positions[i].cpu())\n",
        "                    F1 += compute_f1(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                    EM += compute_em(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                eval_cnt += samples_in_batch\n",
        "            F1 /= eval_cnt\n",
        "            EM /= eval_cnt\n",
        "            print(\"English eval score: F1:{}, EM:{}\".format(F1, EM))\n",
        "            eng_f1, eng_em = F1, EM\n",
        "\n",
        "            eval_cnt = 0\n",
        "            F1 = 0.0\n",
        "            EM = 0.0\n",
        "            for batch_idx, batch in enumerate(zh_loader):\n",
        "                optim.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "                samples_in_batch = len(input_ids)\n",
        "                for i in range(samples_in_batch):\n",
        "                    predict_start = int(outputs[1][i].argmax().cpu())\n",
        "                    predict_end = int(outputs[2][i].argmax().cpu())\n",
        "                    true_start = int(start_positions[i].cpu())\n",
        "                    true_end = int(end_positions[i].cpu())\n",
        "                    F1 += compute_f1(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                    EM += compute_em(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                eval_cnt += samples_in_batch\n",
        "            F1 /= eval_cnt\n",
        "            EM /= eval_cnt\n",
        "            print(\"Chinese eval score: F1:{}, EM:{}\".format(F1, EM))\n",
        "            print('collect data: ', eng_f1, eng_em, F1, EM, dummy_index, epoch, learning_rate)\n",
        "            collect.append((eng_f1, eng_em, F1, EM, dummy_index, epoch, learning_rate))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1449\n",
            "144\n",
            "Epoch: 0/3, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.1906445901621307, EM:0.18118466898954705\n",
            "Chinese eval score: F1:0.07257237134085756, EM:0.051587301587301584\n",
            "collect data:  0.1906445901621307 0.18118466898954705 0.07257237134085756 0.051587301587301584 0 0 5e-06\n",
            "Epoch: 0/3, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.2942128158851099, EM:0.22038327526132404\n",
            "Chinese eval score: F1:0.20196747593864367, EM:0.1130952380952381\n",
            "collect data:  0.2942128158851099 0.22038327526132404 0.20196747593864367 0.1130952380952381 300 0 5e-06\n",
            "Epoch: 0/3, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.44795617763824164, EM:0.3423344947735192\n",
            "Chinese eval score: F1:0.29705462129441734, EM:0.1884920634920635\n",
            "collect data:  0.44795617763824164 0.3423344947735192 0.29705462129441734 0.1884920634920635 600 0 5e-06\n",
            "Epoch: 0/3, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.5146267417662432, EM:0.4076655052264808\n",
            "Chinese eval score: F1:0.34943915302615247, EM:0.23214285714285715\n",
            "collect data:  0.5146267417662432 0.4076655052264808 0.34943915302615247 0.23214285714285715 900 0 5e-06\n",
            "Epoch: 0/3, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.5665092821389839, EM:0.45034843205574915\n",
            "Chinese eval score: F1:0.38069479145578927, EM:0.251984126984127\n",
            "collect data:  0.5665092821389839 0.45034843205574915 0.38069479145578927 0.251984126984127 1200 0 5e-06\n",
            "Epoch: 0/3, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.5824468952171361, EM:0.4686411149825784\n",
            "Chinese eval score: F1:0.4021046802819602, EM:0.27976190476190477\n",
            "collect data:  0.5824468952171361 0.4686411149825784 0.4021046802819602 0.27976190476190477 1448 0 5e-06\n",
            "Epoch: 1/3, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.5803972573227733, EM:0.46602787456445993\n",
            "Chinese eval score: F1:0.4037065227525965, EM:0.27976190476190477\n",
            "collect data:  0.5803972573227733 0.46602787456445993 0.4037065227525965 0.27976190476190477 1449 1 5e-06\n",
            "Epoch: 1/3, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.6114496356080504, EM:0.49390243902439024\n",
            "Chinese eval score: F1:0.3949203139977074, EM:0.2757936507936508\n",
            "collect data:  0.6114496356080504 0.49390243902439024 0.3949203139977074 0.2757936507936508 1749 1 5e-06\n",
            "Epoch: 1/3, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.6154514003773451, EM:0.4991289198606272\n",
            "Chinese eval score: F1:0.40855738871521174, EM:0.2857142857142857\n",
            "collect data:  0.6154514003773451 0.4991289198606272 0.40855738871521174 0.2857142857142857 2049 1 5e-06\n",
            "Epoch: 1/3, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.632933885103377, EM:0.514808362369338\n",
            "Chinese eval score: F1:0.4184441102094067, EM:0.2996031746031746\n",
            "collect data:  0.632933885103377 0.514808362369338 0.4184441102094067 0.2996031746031746 2349 1 5e-06\n",
            "Epoch: 1/3, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.6311219660898255, EM:0.5087108013937283\n",
            "Chinese eval score: F1:0.4155186822273783, EM:0.2916666666666667\n",
            "collect data:  0.6311219660898255 0.5087108013937283 0.4155186822273783 0.2916666666666667 2649 1 5e-06\n",
            "Epoch: 1/3, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.641288510932243, EM:0.5270034843205574\n",
            "Chinese eval score: F1:0.43016260614165547, EM:0.2996031746031746\n",
            "collect data:  0.641288510932243 0.5270034843205574 0.43016260614165547 0.2996031746031746 2897 1 5e-06\n",
            "Epoch: 2/3, batch: 0/1449, 0.000000%\n",
            "English eval score: F1:0.6421219358441035, EM:0.5278745644599303\n",
            "Chinese eval score: F1:0.4310930932790391, EM:0.2996031746031746\n",
            "collect data:  0.6421219358441035 0.5278745644599303 0.4310930932790391 0.2996031746031746 2898 2 5e-06\n",
            "Epoch: 2/3, batch: 300/1449, 20.703934%\n",
            "English eval score: F1:0.6410411570259616, EM:0.5209059233449478\n",
            "Chinese eval score: F1:0.44389587527685564, EM:0.30952380952380953\n",
            "collect data:  0.6410411570259616 0.5209059233449478 0.44389587527685564 0.30952380952380953 3198 2 5e-06\n",
            "Epoch: 2/3, batch: 600/1449, 41.407867%\n",
            "English eval score: F1:0.6391404108162707, EM:0.5217770034843205\n",
            "Chinese eval score: F1:0.43851763887479, EM:0.2996031746031746\n",
            "collect data:  0.6391404108162707 0.5217770034843205 0.43851763887479 0.2996031746031746 3498 2 5e-06\n",
            "Epoch: 2/3, batch: 900/1449, 62.111801%\n",
            "English eval score: F1:0.658205878389144, EM:0.5470383275261324\n",
            "Chinese eval score: F1:0.44217414182796894, EM:0.2996031746031746\n",
            "collect data:  0.658205878389144 0.5470383275261324 0.44217414182796894 0.2996031746031746 3798 2 5e-06\n",
            "Epoch: 2/3, batch: 1200/1449, 82.815735%\n",
            "English eval score: F1:0.665078216730609, EM:0.5522648083623694\n",
            "Chinese eval score: F1:0.4484935824142671, EM:0.31547619047619047\n",
            "collect data:  0.665078216730609 0.5522648083623694 0.4484935824142671 0.31547619047619047 4098 2 5e-06\n",
            "Epoch: 2/3, batch: 1448/1449, 99.930987%\n",
            "English eval score: F1:0.6596279150907215, EM:0.5470383275261324\n",
            "Chinese eval score: F1:0.43402785949247097, EM:0.2996031746031746\n",
            "collect data:  0.6596279150907215 0.5470383275261324 0.43402785949247097 0.2996031746031746 4346 2 5e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD1g0DRYG2CV"
      },
      "source": [
        "with open('MUSE_lr_' + str(learning_rate) + '.csv', 'w') as f:\n",
        "    f.write(\"En_F1,En_EM,Zh_F1,Zh_EM,batch_size,epoch,lr\\n\")\n",
        "    for line in collect:\n",
        "        sline = list(map(lambda x: str(x), line))\n",
        "        sline = ','.join(sline)\n",
        "        f.write(sline+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}