{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "BERT_MLQA_TAR.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83bbf02701874d0ab3a6a54c3ebfa66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c8ac9a3acb44d5fa2839a6304b8ca55",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7148c929e5bd46f7bfe8f8c85220546d",
              "IPY_MODEL_9f7f9f3e7a8e41fcb429d0bd6ae995b4"
            ]
          }
        },
        "5c8ac9a3acb44d5fa2839a6304b8ca55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7148c929e5bd46f7bfe8f8c85220546d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_09f6ee6511b5458fbcc14f3dd24a6445",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f06b192ca7247d195afac1b82e00b3c"
          }
        },
        "9f7f9f3e7a8e41fcb429d0bd6ae995b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ae1e0b284f4c4981bbec0b865df621e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:31&lt;00:00, 19.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5b420ffdab13490a831c95992b2ea688"
          }
        },
        "09f6ee6511b5458fbcc14f3dd24a6445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f06b192ca7247d195afac1b82e00b3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae1e0b284f4c4981bbec0b865df621e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5b420ffdab13490a831c95992b2ea688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8cf88625cdec4978a37370008cb26cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c5c3fbb188a1485eae1e3a4e762c675d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_384920bedb9845f5ac4ab97765d8131c",
              "IPY_MODEL_923cc713a4c8430683927ef368465a89"
            ]
          }
        },
        "c5c3fbb188a1485eae1e3a4e762c675d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "384920bedb9845f5ac4ab97765d8131c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3d5dc35fcc17426d89a45ee3ca40d03e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bae40d3928e846009e3201774cf79aea"
          }
        },
        "923cc713a4c8430683927ef368465a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_21297c9ccbfb48a7b32154fe16be5b5d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 714M/714M [00:30&lt;00:00, 23.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4da7e1450a48446caeddcb1310160ace"
          }
        },
        "3d5dc35fcc17426d89a45ee3ca40d03e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bae40d3928e846009e3201774cf79aea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21297c9ccbfb48a7b32154fe16be5b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4da7e1450a48446caeddcb1310160ace": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJB7yPklBIiD",
        "outputId": "7d0a48f2-5f44-4a5a-a6df-ead1c7139f51"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GwdgrZqBDAr",
        "outputId": "853ddc5d-1d74-411a-a12c-0e989d9eedbf"
      },
      "source": [
        "!pip install gdown\n",
        "!sudo apt-get install unzip\n",
        "!gdown https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip\n",
        "!unzip MLQA_V1.zip\n",
        "!rm MLQA_V1.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Downloading...\n",
            "From: https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip\n",
            "To: /content/MLQA_V1.zip\n",
            "100% 75.7M/75.7M [00:01<00:00, 67.8MB/s]\n",
            "Archive:  MLQA_V1.zip\n",
            "replace MLQA_V1/dev/dev-context-ar-question-ar.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-ar-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-de-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-en-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-es-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-hi-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-vi-question-zh.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-ar.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-de.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-en.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-es.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-hi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-vi.json  \n",
            "  inflating: MLQA_V1/dev/dev-context-zh-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-ar-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-de-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-en-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-es-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-hi-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-vi-question-zh.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-ar.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-de.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-en.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-es.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-hi.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-vi.json  \n",
            "  inflating: MLQA_V1/test/test-context-zh-question-zh.json  \n",
            "  inflating: MLQA_V1/LICENSE         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB-Ob8p9AvKz",
        "outputId": "e82bd79e-2525-498b-f45c-a2ba198c64c0"
      },
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def translate_a_list(sentences):\n",
        "    batch_size = 30\n",
        "    n_sentences = len(sentences) \n",
        "    n_batch = int((n_sentences-1) / batch_size)\n",
        "    result = []\n",
        "    for i in range(n_batch+1):\n",
        "#         print(\"{}/{}: {:2%}\".format(i, n_batch+1, i/(n_batch+1)))\n",
        "        to_translate = sentences[i*batch_size : min((i+1)*batch_size, n_sentences)]\n",
        "        tokenized = tokenizer.prepare_seq2seq_batch(to_translate).to('cuda')\n",
        "        translated = model.generate(**tokenized)\n",
        "        tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "        result += tgt_text\n",
        "    return result\n",
        "\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name).to('cuda')\n",
        "\n",
        "sentences = ['What\\'s the weather like today?', 'How are you?'] \n",
        "print(sentences)\n",
        "print(translate_a_list(sentences))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"What's the weather like today?\", 'How are you?']\n",
            "['今天天气怎么样?', '你好吗?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeDgVJ6wAvKz",
        "outputId": "563440d3-0c80-4246-97d6-c884a2d92658"
      },
      "source": [
        "import json\n",
        "def read_MLQA_v2(path):\n",
        "    with open(path, 'r') as f:\n",
        "        MLQA_dict = json.load(f)\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    \n",
        "    context_idx = 0\n",
        "    for group in MLQA_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            contexts.append(context)\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    answer['context_idx'] = context_idx\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "            context_idx += 1\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "train_contexts_v2, train_questions_v2, train_answers_v2 = read_MLQA_v2('MLQA_V1/test/test-context-en-question-en.json')\n",
        "val_contexts_v2, val_questions_v2, val_answers_v2 = read_MLQA_v2('MLQA_V1/dev/dev-context-en-question-en.json')\n",
        "print(len(train_contexts_v2))\n",
        "print(len(train_questions_v2))\n",
        "print(train_answers_v2[1234])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9916\n",
            "11590\n",
            "{'text': 'obesity, prolonged sitting, a chronic cough, and pelvic floor dysfunction', 'answer_start': 449, 'context_idx': 955}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOR_sBllAvKz",
        "outputId": "6bb9d1b1-8818-4c22-b385-474f74c6463e"
      },
      "source": [
        "train_contexts_v2_zh = translate_a_list(train_contexts_v2)\n",
        "print(train_contexts_v2_zh[8])\n",
        "print(train_contexts_v2[8])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "某些类型的网络交通可能希望或需要明确的服务质量,例如:\n",
            "A defined quality of service may be desired or required for certain types of network traffic, for example:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGE2K3phAvK0"
      },
      "source": [
        "train_questions_v2_zh = translate_a_list(train_questions_v2) # it takes a tremendous time"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9LCRv5cAvK0"
      },
      "source": [
        "train_answers_v2_plain = [answer['text'] for answer in train_answers_v2]\n",
        "train_answers_v2_plain_zh = translate_a_list(train_answers_v2_plain)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhmiB0cNAvK0",
        "outputId": "d4873ae3-8e99-4237-c030-b27c8b688e04"
      },
      "source": [
        "import json\n",
        "def read_MLQA(path):\n",
        "    with open(path, 'r') as f:\n",
        "        MLQA_dict = json.load(f)\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for group in MLQA_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "val_contexts, val_questions, val_answers = read_MLQA('MLQA_V1/dev/dev-context-en-question-en.json')\n",
        "zh_contexts, zh_questions, zh_answers = read_MLQA('MLQA_V1/dev/dev-context-zh-question-zh.json')\n",
        "train_contexts, train_questions, train_answers = read_MLQA('MLQA_V1/test/test-context-en-question-en.json')\n",
        "print(len(train_contexts))\n",
        "print(len(val_contexts))\n",
        "print(len(zh_contexts))\n",
        "print(val_questions[0], val_answers[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11590\n",
            "1148\n",
            "504\n",
            "Does an infection for Sandflies go away over time? {'text': 'remains infected for its lifetime', 'answer_start': 571}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulrUK2ByAvK0",
        "outputId": "f0d379c2-e361-4e6c-f6db-4b1d726b6f26"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # sometimes squad answers are off by a character or two – fix this\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "        else:\n",
        "            print('****')\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)\n",
        "add_end_idx(zh_answers, zh_contexts)\n",
        "print(val_questions[0], val_answers[0])\n",
        "print(zh_questions[2], zh_answers[2])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Does an infection for Sandflies go away over time? {'text': 'remains infected for its lifetime', 'answer_start': 571, 'answer_end': 604}\n",
            "俄罗斯有多少队获得参赛资格？ {'text': '十支', 'answer_start': 153, 'answer_end': 155}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNi2_0pmAvK0"
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
        "zh_encodings = tokenizer(zh_contexts, zh_questions, truncation=True, padding=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3gzGJTWAvK0"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "        # if None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)\n",
        "add_token_positions(zh_encodings, zh_answers)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLqW-ay6AvK0"
      },
      "source": [
        "# GET contexts_token_en_int to align\n",
        "n_questions = len(train_questions)\n",
        "contexts_token_en_int = []\n",
        "for i in range(n_questions):\n",
        "    context_idx = train_answers_v2[i]['context_idx']\n",
        "    if context_idx != len(contexts_token_en_int):\n",
        "        continue\n",
        "    context_token = []\n",
        "    for tok in train_encodings['input_ids'][i][1:]:\n",
        "        if tok != 102:\n",
        "            context_token.append(tok)\n",
        "        else:\n",
        "            break\n",
        "    contexts_token_en_int.append(context_token)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbyC0RqEAvK0"
      },
      "source": [
        "train_contexts_v2_zh[2896]='吡啶在己烷中的光吸收光谱在195 nm(π→π*跃迁,摩尔吸收率ε= 7500 L·mol-1·cm-1),251 nm(π→π*跃迁,ε)的波长处包含三个谱带= 2000 L·mol-1·cm-1)和270 nm(n→π*跃迁,ε= 450 L·mol-1·cm-1).吡啶的1H核磁共振(NMR)光谱包含三个信号,其积分强度比为2：1：2,与分子中的三个化学上不同的质子相对应.这些信号来自α质子(位置2和6,化学位移8.5 ppm),γ质子(位置4，7.5 ppm)和β质子(位置3和5,7.1 ppm).吡啶的碳类似物苯只有7.27 ppm的质子信号.与苯相比,α和γ质子的化学位移较大,这是由于α和γ位置的电子密度较低,这可以从共振结构得出.吡啶和苯的13C NMR谱图的情况非常相似:吡啶在δ(α-C)= 150 ppm,δ(β-C)= 124 ppm和δ(γ-C)= 136 ppm时显示三重态,而苯在129 ppm处有一条单线.所有班次均引用了无溶剂物质.吡啶通常通过气相色谱法和质谱法检测.'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtMkAxdqAvK0",
        "outputId": "c649ad84-579d-40ed-a359-4bbca3bb9c5b"
      },
      "source": [
        "# Step 1.get full contexts for every qa\n",
        "train_contexts_v2_zh_full = []\n",
        "for i in range(n_questions):\n",
        "    train_contexts_v2_zh_full.append(train_contexts_v2_zh[train_answers_v2[i]['context_idx']])\n",
        "\n",
        "# Step 2.tokenize the chinese sentence\n",
        "train_encodings_zh = tokenizer(train_contexts_v2_zh_full, train_questions_v2_zh, truncation=True, padding=True)\n",
        "\n",
        "# Step 3.get contexts_token_zh_int\n",
        "contexts_token_zh_int = []\n",
        "for i in range(n_questions):\n",
        "    context_idx = train_answers_v2[i]['context_idx']\n",
        "    if context_idx != len(contexts_token_zh_int):\n",
        "        continue\n",
        "    context_token = []\n",
        "    for tok in train_encodings_zh['input_ids'][i][1:]:\n",
        "        if tok != 102:\n",
        "            context_token.append(tok)\n",
        "        else:\n",
        "            break\n",
        "    contexts_token_zh_int.append(context_token)\n",
        "    \n",
        "print(len(contexts_token_zh_int[0]))\n",
        "print(len(contexts_token_en_int[0]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "354\n",
            "289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDSPDgVyCGKu",
        "outputId": "da45a6c2-7de9-401e-f25c-95c19601a993"
      },
      "source": [
        "! git clone https://github.com/clab/fast_align.git\n",
        "! sudo apt - get install libgoogle - perftools - dev libsparsehash - dev\n",
        "% cd fast_align\n",
        "! mkdir build\n",
        "% cd build\n",
        "! cmake ..\n",
        "! make"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fast_align'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 213 (delta 2), reused 4 (delta 2), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (213/213), 70.68 KiB | 7.85 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "E: Invalid operation get\n",
            "/content/fast_align\n",
            "/content/fast_align/build\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/fast_align/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target atools\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/alignment_io.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/atools.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable atools\u001b[0m\n",
            "[ 50%] Built target atools\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fast_align\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/fast_align.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/ttables.cc.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable fast_align\u001b[0m\n",
            "[100%] Built target fast_align\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQJubVQfAvK0"
      },
      "source": [
        "# %cd fast_align/build\n",
        "def combine_token_lists(en_token, zh_token):\n",
        "    n = len(en_token)\n",
        "    result = []\n",
        "    for i in range(n):\n",
        "        en_i = en_token[i]\n",
        "        zh_i = zh_token[i]\n",
        "        one_combination = [str(tok) for tok in en_i] + ['|||'] + [str(tok) for tok in zh_i]\n",
        "        result.append(' '.join(one_combination) + '\\n')\n",
        "    return result\n",
        "\n",
        "\n",
        "en_token = contexts_token_en_int\n",
        "zh_token = contexts_token_zh_int    \n",
        "input_of_fastalign = combine_token_lists(en_token, zh_token)\n",
        "with open('to_align.txt', 'w') as f:\n",
        "    f.writelines(input_of_fastalign)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "R_7Xs8ZJ-dAt",
        "outputId": "de95ddee-3831-4b55-9732-d2a940c77ee8"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/fast_align/build'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaAaVMy0AvK0",
        "outputId": "5c14fbd2-8f2e-4d15-c882-373cba546cf3"
      },
      "source": [
        "! ./fast_align -i to_align.txt -d -o -v > forward.align"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ARG=i\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=v\n",
            "INITIAL PASS \n",
            ".........\n",
            "expected target length = source length * 1.1595\n",
            "ITERATION 1\n",
            ".........\n",
            "  log_e likelihood: -4.30224e+07\n",
            "  log_2 likelihood: -6.20682e+07\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.162334\n",
            "       size counts: 8301\n",
            "ITERATION 2\n",
            ".........\n",
            "  log_e likelihood: -1.48968e+07\n",
            "  log_2 likelihood: -2.14915e+07\n",
            "     cross entropy: 10.3521\n",
            "        perplexity: 1307.08\n",
            "      posterior p0: 0.128264\n",
            " posterior al-feat: -0.142384\n",
            "       size counts: 8301\n",
            "  1  model al-feat: -0.166956 (tension=4)\n",
            "  2  model al-feat: -0.155996 (tension=4.49144)\n",
            "  3  model al-feat: -0.150384 (tension=4.76368)\n",
            "  4  model al-feat: -0.147231 (tension=4.92368)\n",
            "  5  model al-feat: -0.145371 (tension=5.02062)\n",
            "  6  model al-feat: -0.144243 (tension=5.08036)\n",
            "  7  model al-feat: -0.143549 (tension=5.11756)\n",
            "  8  model al-feat: -0.143116 (tension=5.14085)\n",
            "     final tension: 5.1555\n",
            "ITERATION 3\n",
            ".........\n",
            "  log_e likelihood: -1.25334e+07\n",
            "  log_2 likelihood: -1.80819e+07\n",
            "     cross entropy: 8.7098\n",
            "        perplexity: 418.708\n",
            "      posterior p0: 0.0777692\n",
            " posterior al-feat: -0.119959\n",
            "       size counts: 8301\n",
            "  1  model al-feat: -0.142845 (tension=5.1555)\n",
            "  2  model al-feat: -0.134786 (tension=5.61323)\n",
            "  3  model al-feat: -0.129962 (tension=5.90978)\n",
            "  4  model al-feat: -0.126871 (tension=6.10984)\n",
            "  5  model al-feat: -0.124808 (tension=6.24808)\n",
            "  6  model al-feat: -0.123396 (tension=6.34507)\n",
            "  7  model al-feat: -0.122412 (tension=6.41381)\n",
            "  8  model al-feat: -0.121718 (tension=6.46287)\n",
            "     final tension: 6.49805\n",
            "ITERATION 4\n",
            ".........\n",
            "  log_e likelihood: -1.17439e+07\n",
            "  log_2 likelihood: -1.69428e+07\n",
            "     cross entropy: 8.16111\n",
            "        perplexity: 286.245\n",
            "      posterior p0: 0.068716\n",
            " posterior al-feat: -0.10332\n",
            "       size counts: 8301\n",
            "  1  model al-feat: -0.121224 (tension=6.49805)\n",
            "  2  model al-feat: -0.116397 (tension=6.85613)\n",
            "  3  model al-feat: -0.113084 (tension=7.11767)\n",
            "  4  model al-feat: -0.11072 (tension=7.31295)\n",
            "  5  model al-feat: -0.108988 (tension=7.46095)\n",
            "  6  model al-feat: -0.107694 (tension=7.5743)\n",
            "  7  model al-feat: -0.106715 (tension=7.66178)\n",
            "  8  model al-feat: -0.105966 (tension=7.72967)\n",
            "     final tension: 7.78259\n",
            "ITERATION 5 (FINAL)\n",
            ".........\n",
            "  log_e likelihood: -1.14906e+07\n",
            "  log_2 likelihood: -1.65774e+07\n",
            "     cross entropy: 7.98507\n",
            "        perplexity: 253.364\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 8301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLeeBYTJAvK0",
        "outputId": "860d349a-5963-40f1-a7e5-ea45fb15d577"
      },
      "source": [
        "# Step 1: Convert chinese answer to token\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "answer_token_zh_int_mid = tokenizer(train_answers_v2_plain_zh, truncation=True, padding=True)\n",
        "answer_token_zh_int = []\n",
        "for answer in answer_token_zh_int_mid['input_ids']:\n",
        "    l = answer\n",
        "    res = l[l.index(101) + 1: l.index(102)]\n",
        "    answer_token_zh_int.append(res)\n",
        "\n",
        "# Step 2: Find index of tokens for english answers\n",
        "# range is [a,b]\n",
        "answer_source_start_token = []\n",
        "answer_source_end_token = []\n",
        "for i in range(n_questions):\n",
        "    s = train_encodings['start_positions'][i] - 1\n",
        "    t = train_encodings['end_positions'][i] - 1\n",
        "    answer_source_start_token.append(s)\n",
        "    answer_source_end_token.append(t)\n",
        "\n",
        "# Step 3: Build context mapping dict from aligned file\n",
        "context_mappings = []\n",
        "with open('forward.align', 'r') as f:\n",
        "    for line in f:\n",
        "        context_mapping = {}\n",
        "        maps = line.strip().split()\n",
        "        for m in maps:\n",
        "            src, tgt = m.split('-')\n",
        "            src = int(src)\n",
        "            tgt = int(tgt)\n",
        "            if src in context_mapping.keys():\n",
        "                context_mapping[src].append(tgt)\n",
        "            else:\n",
        "                context_mapping[src] = [tgt]\n",
        "        context_mappings.append(context_mapping)\n",
        "\n",
        "print(len(context_mappings))\n",
        "print(len(train_contexts_v2))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9916\n",
            "9916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyOSqLiDAvK0",
        "outputId": "c8c31d55-90a1-424f-efb1-e55ac13fb260"
      },
      "source": [
        "answer_target_start_token_possible = []\n",
        "answer_target_end_token_possible = []\n",
        "\n",
        "star_cnt = 0\n",
        "for i in range(n_questions):\n",
        "    s_start = answer_source_start_token[i]\n",
        "    s_end = answer_source_end_token[i]\n",
        "    t_start = 10000\n",
        "    t_end = -1\n",
        "    context_idx = train_answers_v2[i]['context_idx']\n",
        "    context_mapping = context_mappings[context_idx]\n",
        "    for j in range(s_start, s_end + 1):\n",
        "        if j in context_mapping:\n",
        "            for tgt in context_mapping[j]:\n",
        "                t_start = min(t_start, tgt)\n",
        "                t_end = max(t_end, tgt)\n",
        "    if t_start == 10000 or t_end == -1:\n",
        "        star_cnt += 1\n",
        "        t_start = 0\n",
        "        t_end = -1\n",
        "    answer_target_start_token_possible.append(t_start)\n",
        "    answer_target_end_token_possible.append(t_end)\n",
        "    \n",
        "print(star_cnt)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS50QY6qAvK0",
        "outputId": "4bd68298-4de5-4ada-f5f7-476bb261f1a3"
      },
      "source": [
        "def find_sublist(l, sub):\n",
        "    l1 = len(l)\n",
        "    l2 = len(sub)\n",
        "    for i in range(0, l1-l2+1):\n",
        "        suc = True\n",
        "        for j in range(l2):\n",
        "            if l[i+j] != sub[j]:\n",
        "                suc = False\n",
        "                break\n",
        "        if suc:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "answer_target_start_token = []\n",
        "answer_target_end_token = []\n",
        "small_index = []\n",
        "for i in range(n_questions):\n",
        "    context_idx = train_answers_v2[i]['context_idx']\n",
        "#     print(i)\n",
        "    answer_token = answer_token_zh_int[i]\n",
        "    start_possible = answer_target_start_token_possible[i]\n",
        "    end_possible = answer_target_end_token_possible[i]\n",
        "    \n",
        "    context_token = contexts_token_zh_int[context_idx]\n",
        "    context_token_ranged = context_token[start_possible:]\n",
        "    \n",
        "    retrived = find_sublist(context_token_ranged, answer_token)\n",
        "    if retrived == -1:\n",
        "        answer_target_start_token.append(start_possible)\n",
        "        answer_target_end_token.append(end_possible)\n",
        "    else:\n",
        "        answer_target_start_token.append(retrived + start_possible)\n",
        "        answer_target_end_token.append(retrived + start_possible + len(answer_token) - 1)\n",
        "        small_index.append(i)\n",
        "\n",
        "print(len(small_index))\n",
        "print(len(answer_target_start_token))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2394\n",
            "11590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP3DpS0JAvK0"
      },
      "source": [
        "answer_target_start_token_to_add = [p + 1 for p in answer_target_start_token]\n",
        "answer_target_end_token_to_add = [p + 1 for p in answer_target_end_token]\n",
        "\n",
        "train_encodings_zh.update({'start_positions': answer_target_start_token_to_add, 'end_positions': answer_target_end_token_to_add})"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuuprkHXAvK1",
        "outputId": "4f4da4be-4e83-4454-9f84-cdcbc057ffde"
      },
      "source": [
        "def merge_encoding(e1, e2):\n",
        "    em = {}\n",
        "    keys = e1.keys()\n",
        "    for k in keys:\n",
        "        em[k] = e1[k] + e2[k]\n",
        "    return em\n",
        "\n",
        "def select_encodings(e, s):\n",
        "    keys = e.keys()\n",
        "    res = {}\n",
        "    for k in keys:\n",
        "        new_value = []\n",
        "        for selected in s:\n",
        "            new_value.append(e[k][selected])\n",
        "        res[k] = new_value\n",
        "    return res\n",
        "\n",
        "train_small_encodings_zh = select_encodings(train_encodings_zh, small_index)\n",
        "train_small = merge_encoding(train_encodings, train_small_encodings_zh)\n",
        "train_large = merge_encoding(train_encodings, train_encodings_zh)\n",
        "print(train_large.keys())\n",
        "print(len(train_large['end_positions']))\n",
        "print(len(train_small['end_positions']))\n",
        "print(len(train_encodings['end_positions']))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
            "23180\n",
            "13984\n",
            "11590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2idibH1AvK1"
      },
      "source": [
        "import torch\n",
        "\n",
        "class MLQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "train_dataset = MLQADataset(train_encodings)\n",
        "val_dataset = MLQADataset(val_encodings)\n",
        "zh_dataset = MLQADataset(zh_encodings)\n",
        "train_zh_dataset = MLQADataset(train_encodings_zh)\n",
        "train_large_dataset = MLQADataset(train_large)\n",
        "train_small_dataset = MLQADataset(train_small)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pGv92DwAvK1",
        "outputId": "56e3c2f7-dd15-4f2f-e7d3-e3e1925f0352"
      },
      "source": [
        "%pwd\n",
        "%cd ../.."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_n6T8rrAvK1"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"train_large_dataset\", \"wb\") as f:\n",
        "    pickle.dump(train_large_dataset, f)\n",
        "\n",
        "with open(\"train_dataset\", \"wb\") as f:\n",
        "    pickle.dump(train_dataset, f)\n",
        "\n",
        "with open(\"val_dataset\", \"wb\") as f:\n",
        "    pickle.dump(val_dataset, f)\n",
        "    \n",
        "with open(\"zh_dataset\", \"wb\") as f:\n",
        "    pickle.dump(zh_dataset, f)\n",
        "\n",
        "with open(\"train_zh_dataset\", \"wb\") as f:\n",
        "    pickle.dump(train_zh_dataset, f)\n",
        "    \n",
        "with open(\"train_small_dataset\", \"wb\") as f:\n",
        "    pickle.dump(train_small_dataset, f)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RQM1gEPAvK1"
      },
      "source": [
        "def compute_f1(predicted, true):\n",
        "    c = len(set(predicted) & set(true))\n",
        "    l1 = len(predicted)\n",
        "    l2 = len(true)\n",
        "    if(l1 + l2 == 0):\n",
        "        return 1\n",
        "    f1 = 2*c/(l1+l2)\n",
        "    return f1\n",
        "    \n",
        "def compute_em(predicted, true):\n",
        "    return int(predicted == true)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "83bbf02701874d0ab3a6a54c3ebfa66a",
            "5c8ac9a3acb44d5fa2839a6304b8ca55",
            "7148c929e5bd46f7bfe8f8c85220546d",
            "9f7f9f3e7a8e41fcb429d0bd6ae995b4",
            "09f6ee6511b5458fbcc14f3dd24a6445",
            "7f06b192ca7247d195afac1b82e00b3c",
            "ae1e0b284f4c4981bbec0b865df621e8",
            "5b420ffdab13490a831c95992b2ea688",
            "8cf88625cdec4978a37370008cb26cea",
            "c5c3fbb188a1485eae1e3a4e762c675d",
            "384920bedb9845f5ac4ab97765d8131c",
            "923cc713a4c8430683927ef368465a89",
            "3d5dc35fcc17426d89a45ee3ca40d03e",
            "bae40d3928e846009e3201774cf79aea",
            "21297c9ccbfb48a7b32154fe16be5b5d",
            "4da7e1450a48446caeddcb1310160ace"
          ]
        },
        "id": "wFjgZvaaAvK1",
        "outputId": "5e4d3661-1dcb-48ec-8cc1-dfc80ae8eb29"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, BertForQuestionAnswering\n",
        "# input_ids = None\n",
        "# attention_mask = None\n",
        "# start_positions = None\n",
        "# end_positions = None\n",
        "# model = None\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_large_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "zh_loader = DataLoader(zh_dataset, batch_size=8, shuffle=True)\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "\n",
        "val_batch = 300\n",
        "max_epoch = 3\n",
        "train_batch = len(train_loader)\n",
        "for epoch in range(max_epoch):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        model.train()\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        if batch_idx % val_batch == 0 or batch_idx == train_batch - 1:\n",
        "            print(\"Epoch: {}/{}, batch: {}/{}, {:%}\".format(epoch, max_epoch, batch_idx, train_batch, batch_idx/train_batch))\n",
        "            model.eval()\n",
        "            eval_cnt = 0\n",
        "            F1 = 0.0\n",
        "            EM = 0.0\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                optim.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "                samples_in_batch = len(input_ids)\n",
        "                for i in range(samples_in_batch):\n",
        "                    predict_start = int(outputs[1][i].argmax().cpu())\n",
        "                    predict_end = int(outputs[2][i].argmax().cpu())\n",
        "                    true_start = int(start_positions[i].cpu())\n",
        "                    true_end = int(end_positions[i].cpu())\n",
        "                    F1 += compute_f1(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                    EM += compute_em(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                eval_cnt += samples_in_batch\n",
        "            F1 /= eval_cnt\n",
        "            EM /= eval_cnt\n",
        "            print(\"English eval score: F1:{}, EM:{}\".format(F1, EM))\n",
        "            \n",
        "            eval_cnt = 0\n",
        "            F1 = 0.0\n",
        "            EM = 0.0\n",
        "            for batch_idx, batch in enumerate(zh_loader):\n",
        "                optim.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "                samples_in_batch = len(input_ids)\n",
        "                for i in range(samples_in_batch):\n",
        "                    predict_start = int(outputs[1][i].argmax().cpu())\n",
        "                    predict_end = int(outputs[2][i].argmax().cpu())\n",
        "                    true_start = int(start_positions[i].cpu())\n",
        "                    true_end = int(end_positions[i].cpu())\n",
        "                    F1 += compute_f1(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                    EM += compute_em(range(predict_start, predict_end), range(true_start, true_end))\n",
        "                eval_cnt += samples_in_batch\n",
        "            F1 /= eval_cnt\n",
        "            EM /= eval_cnt\n",
        "            print(\"Chinese eval score: F1:{}, EM:{}\".format(F1, EM))\n",
        "            \n",
        "        \n",
        "        \n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83bbf02701874d0ab3a6a54c3ebfa66a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cf88625cdec4978a37370008cb26cea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-51ee95128336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-multilingual-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqa_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0madd_start_docstrings_to_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBERT_INPUTS_DOCSTRING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size, sequence_length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36minit_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \"\"\"\n\u001b[1;32m    672\u001b[0m         \u001b[0;31m# Initialize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# Prune heads if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \"\"\"\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \"\"\"\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \"\"\"\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36m_init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;31m# Slightly different from the TF version which uses truncated_normal for initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;31m# cf https://github.com/pytorch/pytorch/pull/5617\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HKUIL3PqAvK1"
      },
      "source": [
        "# FOR TESTING ONLY IGNORE\n",
        "import json\n",
        "with open(\"MLQA_V1/dev/dev-context-en-question-en.json\",'r') as load_f:\n",
        "    load_dict = json.load(load_f)\n",
        "    print(load_dict.keys())\n",
        "    print(len(load_dict['data']))\n",
        "    print(load_dict['data'][0].keys())\n",
        "    print(load_dict['data'][0]['title'])\n",
        "    print(load_dict['data'][0]['paragraphs'])\n",
        "    print(load_dict['data'][0]['paragraphs'][0].keys())\n",
        "#     print(load_dict['data'][0]['paragraphs'][0]['context'])\n",
        "    print(load_dict['data'][0]['paragraphs'][0]['qas'])\n",
        "    print(load_dict['data'][0]['paragraphs'][0]['qas'][0])\n",
        "    \n",
        "#TODO:(about code)\n",
        "# 1. baseline Evaluation metric, Train faster (finished)\n",
        "# 1.5 need to compare eng and chinese\n",
        "# 2. word-level translate and generate new dataset of another language\n",
        "# 3. sentence-level translate and alignment + retrive(min, max as tar) \n",
        "\n",
        "# 可能 \n",
        "# 输入：英文句子+中文句子（乱序） \n",
        "# 输出：单词的对应关系\n",
        "\n",
        "# English eval score: F1:0.6419080228993544, EM:0.5287456445993032 -- MLQA. M-BERT\n",
        "# Chinese eval score: F1:0.42465514513374364, EM:0.2996031746031746\n",
        "true    predicted\n",
        "5 - 10  6- 11\n",
        "6-10 intersection\n",
        "6-10/5-10 recall\n",
        "6-10/6-11 precision"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}